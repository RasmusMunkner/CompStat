---
title: "Stochastic Optimization"
author: "Rasmus V. Munkner"
format: revealjs
slide-number: c/t
---

## Relevant R-Packages

```{r Loading libraries}
#| echo: true

library(CompStat) # My implementation is bundled as a package
library(testthat) # For formal testing
library(microbenchmark) # Benchmarking

```

## A matrix derivation of the relevant likelihood{style="font-size: 50%"}
Consider the negative loglikelihood
$$
H(\beta) =
- \frac{1}{N}\sum_{i=1}^N y_i \log(p_i(\beta)) + (1-y_i)\log(1-p_i(\beta)) + \lambda||f_\beta''||_2^2 :=
g(p(\beta)) + \lambda||f_\beta''||_2^2
$$
with $g: \mathbb{R}^N \rightarrow \mathbb{R}$ and $p: \mathbb{R}^p \rightarrow \mathbb{R}^n$. It holds that
$$
D_\beta H(\beta) = D_p(g(p(\beta))) \cdot D_\beta p(\beta) + \lambda D_\beta||f_\beta''||_2^2
$$

Note that
$$
p(\beta) = \frac{e^{X\beta}}{1+e^{X\beta}} \\
D_\beta p(\beta) = \text{diag}\left(\frac{e^{X\beta}}{(1+e^{X\beta})^2}\right)X
$$

And
$$
g(p) = -\frac{1}{N}\textbf{1}^T(y \log(p) + (1-y)\log(1-p)) \\
D_pg(p) = -\frac{1}{N}\textbf{1}^T\cdot\text{diag}\left(\frac{y}{p} - \frac{1-y}{1-p}\right)
$$

## Penalization {style="font-size: 50%"}
For the penalization term
$$
||f''_\beta||_2^2 =
\int f''_\beta(x)^2dx =
\int \left(\sum_{k=1}^p \varphi_k''(x)\beta_k\right)^2 dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j\int \varphi_k(x)''\varphi_j(x)'' dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j \langle\varphi_k'',\varphi_j''\rangle :=
\beta^T \Omega \beta
$$

Thus

$$
D_\beta ||f_\beta''||_2^2 = 2 \beta^T\Omega
$$

This can be seem by utilizing

$$
\beta^T\Omega \beta = \beta^T \Omega^{T/2} \Omega^{1/2} \beta = \xi^T\xi \\
D_\beta(\beta^T\Omega\beta) = D_\xi(\xi^T\xi) \cdot D_\beta(\Omega^{1/2}\beta) = 2\xi^T\Omega^{1/2} = 2\beta^T\Omega^{T/2}\Omega^{1/2} = 2\beta^T\Omega
$$

## Implementation

```r{code-line-numbers="1-6|8-13|48-55|15-20|22-33|34-46"}
logistic_loglikelihood <- function(
    response,
    design,
    penalty_matrix = NULL,
    lambda = 1e-5
){

  if (c("CompStatBasisExpansion") %in% class(design)){
    penalty_matrix <- design$Omega
    design <- design$X
  } else if (is.null(penalty_matrix)){
    penalty_matrix <- diagmat(rep(1,ncol(design))) # Ordinary L2 Penalty
  }

  loglikelihood <- function(coef, batch = 1:nrow(design)){
    eta <- exp(design[batch,] %*% coef)
    p <- eta / (1 + eta)
    -mean(response[batch] * log(p) + (1-response[batch]) * log(1-p)) +
      lambda * t(coef) %*% penalty_matrix %*% coef
  }

  grad <- function(coef, batch = 1:nrow(design)){
    X <- design[batch,, drop=F]
    y <- response[batch]

    eta <- exp(X %*% coef)
    p <- eta / (1 + eta)
    dp <- t(X) %*% diagmat(eta / (1 + eta)^2)
    dg <-  - (y/p - (1-y)/(1-p)) / length(batch)
    grad <- dp %*% dg %>% as.vector() + (2 * lambda * penalty_matrix %*% coef)
    grad %>% as.vector()
  }

  structure(list(
    objective = loglikelihood,
    grad = grad,
    n_param = nrow(penalty_matrix),
    n_index = length(response),
    design = design,
    response = response,
    penalty_matrix = penalty_matrix,
    lambda = lambda
  ),
  class = c("CompStatLogisticLogLikelihood", "CompStatOptimizable")
  )
}

CompStatBasisExpansion <- function(X, x, Omega, expand_new){
  structure(list(
    X = X,
    x = x,
    Omega = Omega,
    expand_new = expand_new
  ), class = "CompStatBasisExpansion")
}

#---
```


## Stochastic Optimization Algorithms
We consider three different stochastic gradient descent algorithms, each of
which share the 'Adam' interface

```r{code-line-numbers="1-3|5-14|16-19|21-25|27-33|35-48|50-58"}
Adam_Optimizer <- function(
    lr = 1e-3, batch_size = 32, beta_1 = 0.95, beta_2 = 0.97, eps = 1e-8, amsgrad = T
    ){

  update_param <- function(grad){
    rho <<- beta_1 * rho + (1-beta_1) * grad
    nu_proposal <- beta_2 * nu + (1-beta_2) * grad^2
    if (amsgrad){
      nu <<- pmax(nu_proposal, nu)
    } else {
      nu <<- nu_proposal
    }
    rho / (sqrt(nu) + eps)
  }

  reset <- function(){
    rho <<- 0
    nu <<- 0
  }

  CompStatOptimizer(
    lr, batch_size, update_param, reset,
    par = list(beta_1 = beta_1, beta_2 = beta_2, eps = eps, amsgrad = amsgrad)
    )
}

Momentum_Optimizer <- function(lr = 1e-3, batch_size = 32, beta_1 = 0.95){
  Adam_Optimizer(lr, batch_size, beta_1 = beta_1, beta_2 = 1, eps = 1)
}

Vanilla_Optimizer <- function(lr = 1e-3, batch_size = 32){
  Adam_Optimizer(lr, batch_size, beta_1 = 0, beta_2 = 1, eps = 1)
}

CompStatOptimizer <- function(lr, batch_size, update_param, reset, par){

  # Ensure the learning rate is callable
  if (is.function(lr)){
    lrate <- lr
  } else {
    lrate <- function(epoch, ...) {lr[pmin(epoch, length(lr))]}
  }
  # Ensure batch_size is callable
  if (is.function(batch_size)){
    bsize <- batch_size
  } else {
    bsize <- function(epoch, ...) {batch_size[pmin(epoch, length(batch_size))]}
  }

  structure(list(
    lr = lrate,
    batch_size = bsize,
    update_param = update_param,
    reset = reset,
    par = par
  ),
  class = "CompStatOptimizer")
}

```

## Assembling a SGD algorithm

```r{code-line-numbers="1-9|11-12|14-15|17-28|30-36|42-50|52-58|60-72|80-89"}
SGD <- function(
    optimizable,
    optimizer = Vanilla_Optimizer(),
    init_par = NULL,
    stop_crit = 50,
    shuffle = T,
    tracing = T,
    seed = NULL
    ){

  dqrng::dqRNGkind("Xoroshiro128+")
  dqrng::dqset.seed(seed)

  # Ensure stopping criterion is valid
  stop_crit <- stopping_criterion(stop_crit)

  # Determine optimizer
  if (!(class(optimizer) %in% c("CompStatOptimizer"))){
    opt <-
      switch(optimizer,
           "vanilla"= Vanilla_Optimizer(),
           "momentum"= Momentum_Optimizer(),
           "adam" = Adam_Optimizer()
           )
  } else {
    opt <- optimizer
  }
  opt$reset()

  # Initialize parameters
  if (is.null(init_par)){
    init_par <- rep(0, optimizable$n_param)
  }
  init_par <- init_par %>% dplyr::coalesce(0)
  par_next <- init_par
  obj_next <- optimizable$objective(init_par)

  trace <- matrix(
    NA, nrow = (stop_crit$maxiter+1), ncol = optimizable$n_param + 1)
  trace[1,] <- c(par_next, optimizable$objective(par_next))

  for (epoch in 1:stop_crit$maxiter){

    # Reshuffle observations
    if (shuffle){
      index_permutation <- dqrng::dqsample.int(
        optimizable$n_index, optimizable$n_index, replace = F)
    } else {
      index_permutation <- 1:optimizable$n_index
    }

    # Remember the previous parameters
    par_before <- par_next
    obj_before <- obj_next

    # Determine batch size
    batch_size <- optimizer$batch_size(epoch, obj = obj_before, n = optimizable$n_index)
    batches_per_epoch <- ceiling(optimizable$n_index / batch_size)

    # Apply minibatch gradient updates
    for (b in 1:batches_per_epoch){

      par_now <- par_next

      grad <- optimizable$grad(
        par_now,
        index_permutation[(1+(b-1)*batch_size):
                            min(b*batch_size, optimizable$n_index)]
      )

      par_next <- par_now - opt$lr(epoch) * opt$update_param(grad)
    }

    # Tracing and keep track of objective function
    obj_next <- optimizable$objective(par_next)
    if (tracing == T){
      trace[epoch+1,] <- c(par_next, obj_next)
    }

    # Check if stopping criterion is fulfilled
    if (stop_crit$check(
      epoch,
      param = par_next,
      param_old = par_before,
      obj = obj_next,
      obj_old = obj_before
    )
    ){
      return(
        if (tracing){
          trace %>%
            magrittr::set_colnames(c(paste0("p", 1:optimizable$n_param), "obj")) %>%
            as.data.frame() %>%
            magrittr::set_class(c("CompStatTrace", class(.))) %>%
            return()
        } else {
          c(par_next, obj_next) %>%
            matrix(nrow = 1) %>%
            magrittr::set_colnames(c(paste0("p", 1:optimizable$n_param), "obj")) %>%
            as.data.frame() %>%
            return()
        }

      )
    }
  }

  stop("An error with maxiter occured. SGD should call return from within the iteration loop.")
}
```

## Checking that everything works

```{r Checking that SGD works}
#| echo: true
#| code-line-numbers: "2-7|8-19|20-22"
test_that("sgd converges to the right values ", {
  set.seed(0)
  targets <- rnorm(10)
  init_par <- rnorm(10)
  sc <- stopping_criterion(maxiter = 250)
  lr <- polynomial_schedule(0.4, 0.05, later=100, p=1)
  opt_target <- optimizable_parabola(targets)
  trace_vanilla <- SGD(
    opt_target, Vanilla_Optimizer(lr),
    init_par = init_par, stop_crit = sc
    )
  trace_momentum <- SGD(
    opt_target, Momentum_Optimizer(lr, beta_1 = 0.8),
    init_par = init_par, stop_crit = sc
  )
  trace_adam <- SGD(
    opt_target, Adam_Optimizer(lr, beta_1 = 0.8),
    init_par = init_par, stop_crit = sc
  )
  expect_equal(max(abs(tail(trace_vanilla) - targets)) < 1e-6, TRUE)
  expect_equal(max(abs(tail(trace_momentum) - targets)) < 1e-6, TRUE)
  expect_equal(max(abs(tail(trace_adam) - targets)) < 1e-6, TRUE)

  set.seed(NULL)
})
```

## Visualizing the results

```{r Checking that SDG works with plots}
set.seed(0)
targets <- rnorm(10)
init_par <- rnorm(10)
sc <- stopping_criterion(maxiter = 250)
lr <- polynomial_schedule(0.4, 0.05, later=100, p=1)
opt_target <- optimizable_parabola(targets)
trace_vanilla <- SGD(
  opt_target, Vanilla_Optimizer(lr),
  init_par = init_par, stop_crit = sc
  )
trace_momentum <- SGD(
  opt_target, Momentum_Optimizer(lr, beta_1 = 0.8),
  init_par = init_par, stop_crit = sc
)
trace_adam <- SGD(
  opt_target, Adam_Optimizer(lr, beta_1 = 0.8),
  init_par = init_par, stop_crit = sc
)
set.seed(NULL)
  
p1 <- trace_vanilla %>% plot("p") + ggplot2::ggtitle("Vanilla")
p2 <- trace_momentum %>% plot("p") + ggplot2::ggtitle("Momentum")
p3 <- trace_adam %>% plot("p") + ggplot2::ggtitle("Adam")

gridExtra::grid.arrange(grobs = list(p1, p2, p3), nrow = 2)
```

## Applying SGD to the horse data

```{r Applying SGD to horse data}
#| cache: true
set.seed(0)
data("horses")
design <- horses$Temperature %>% ExpandBspline()
opt_target_horses <- logistic_loglikelihood(
  horses$dead,
  design = design,
  lambda = 1e-5
  )
lr <- polynomial_schedule(0.1, 0.01, later = 100, p = 1)
adam <- Adam_Optimizer(lr, batch_size = ceiling(nrow(horses)/4))
trace <- SGD(
  opt_target_horses, adam, init_par = rep(0,opt_target_horses$n_param),
  stop_crit = 200,
  seed = 0
  )
set.seed(NULL)

p1 <- plot(trace, "p") + ggplot2::ggtitle("Parameter Values")

p2 <- plot(trace, "o") + ggplot2::ggtitle("Objective Function")

p3 <- plot(design, trace %>% tail(1), post_transform = function(x) exp(x) / (1 + exp(x))) +
  ggplot2::lims(x = c(34,42), y = c(0,1)) +
  ggplot2::labs(x = "Temperature", y = "P(Dead|Temperature)") +
  ggplot2::ggtitle("Final Fitted Regression")

p4 <- ggplot2::ggplot(horses, ggplot2::aes(x = Temperature, y = dead)) +
  ggplot2::geom_smooth(method = "gam", se = F) +
  ggplot2::lims(x = c(34,42), y = c(0,1)) +
  ggplot2::labs(x = "Temperature", y = "GAM smoother (Dead | Temperature)") +
  ggplot2::ggtitle("GAM smoother")

gridExtra::grid.arrange(grobs = list(p1, p2, p3, p4), nrow = 2)

```

## Adam vs. AMSgrad

```{r Adam vs amsgrad}

adam_noams <- Adam_Optimizer(
  lr, batch_size = ceiling(nrow(horses)/4), amsgrad = F)

trace_noams <- SGD(
  opt_target_horses, adam_noams, init_par = rep(0,opt_target_horses$n_param),
  stop_crit = 200,
  seed = 0
  )

p1 <- CompareTraces(list(AMS = trace, Adam = trace_noams), what = "p")
p2 <- CompareTraces(list(AMS = trace, Adam = trace_noams), what = "o")
shift <- 50
p3 <- CompareTraces(
  list(AMS = trace[shift:nrow(trace),], Adam = trace_noams[shift:nrow(trace_noams),]), what = "p", row_nr_shift = shift)
p4 <- CompareTraces(
  list(AMS = trace[shift:nrow(trace),], Adam = trace_noams[shift:nrow(trace_noams),]), what = "o", row_nr_shift = shift)

gridExtra::grid.arrange(grobs = list(p1, p2, p3, p4), ncol = 2)

#CompareTraces(list(AMS = trace[50:nrow(trace),], Adam = trace_noams[50:nrow(trace_noams),]))

```


## Profiling the implementation{.smaller}

```{r Profiling r SGD}
#| cache: true

set.seed(0)
profvis::profvis({
  trace <- SGD(
  opt_target_horses, adam, init_par = rep(0,opt_target$n_param),
  stop_crit = 1000
  )
})
set.seed(NULL)

```

## C++ implementation

```cpp{code-line-numbers="1-9|10-20|22-26|28-40|42-52|54-67|69-74"}
Rcpp::List SGD_CPP_PRIMITIVE(
    const arma::mat &design, arma::vec coef, const arma::vec &y,
    const arma::mat &pen_matrix, const double &lambda,
    const NumericVector &lr, const IntegerVector &batch_size,
    const int &maxiter, const double &objtarget,
    const double &beta_1, const double &beta_2, const double &eps,
    const bool &amsgrad,
    const int &seed
){
  Rcpp::List coef_list (maxiter);
  Rcpp::List obj_list (maxiter);
  double obj;
  arma::vec grad;
  arma::vec rho = vec(coef.n_elem); rho.zeros();
  arma::vec nu = vec(coef.n_elem); nu.zeros();
  int batch_size_now;
  int batches_per_epoch;
  uvec indicies_all, indicies;
  dqrng::dqRNGkind("Xoroshiro128+");
  dqrng::dqset_seed(IntegerVector::create(seed)); // Sets the shuffling seed

  for (int i = 0; i < maxiter; i++){
    indicies_all = as<uvec>(dqrng::dqsample_int(y.n_elem, y.n_elem));

    batch_size_now = std::min<int>(y.n_elem, batch_size[i]);
    batches_per_epoch = std::ceil(double(y.n_elem) / batch_size_now);

    for (int b = 0; b < batches_per_epoch; b++){
      indicies = indicies_all.subvec(
        b * batch_size_now,
        std::min<int>((b+1) * batch_size_now - 1, y.n_elem - 1)
      );

      grad = lll_gradC(
        design.rows(indicies),
        coef,
        y.elem(indicies),
        pen_matrix,
        lambda
        );

      rho = rho * beta_1 + grad * (1 - beta_1);
      if (amsgrad){
        nu = arma::max(
          nu * beta_2 + pow(grad, 2) * (1 - beta_2), nu
        );
      } else {
        nu = nu * beta_2 + pow(grad, 2) * (1 - beta_2);
      }

      coef = coef - lr[i] * rho / (sqrt(nu) + eps);
    }

    coef_list[i] = as<NumericVector>(wrap(coef)); // Breaks mutability of coef
    obj = lllC(
      design,
      coef,
      y,
      pen_matrix,
      lambda
    );
    obj_list[i] = 1 * obj;

    if (obj < objtarget){
      break;
    }
  }

  Rcpp::List results (2);
  results[0] = coef_list;
  results[1] = obj_list;

  return(results);
}
```

## Testing consistency

```{r Testing consistency across R and c++}
#| cache: true
#| echo: true
#| code-line-numbers: "3-5|7-14|16-22|24-39"
test_that("C++ sgd converges to the right value", {

  set.seed(0)
  t <- 10
  p <- 8

  targets <- rnorm(p)
  sll <- simple_logistic_loglikelihood(n = 647, p = p, beta = targets)

  opt_target <- logistic_loglikelihood(
    design = sll$X, response = sll$y,
    penalty_matrix = matrix(0, nrow = ncol(sll$X), ncol = ncol(sll$X)),
    lambda = 0
  )

  random_coef <- 1:t %>% purrr::map(.f = function(i) rnorm(p))

  epochs <- 50
  batch_size <- 24
  lr <- polynomial_schedule(0.1, 0.001, 50)
  opt <- Adam_Optimizer(
    lr, batch_size, beta_1 = 0.9, beta_2 = 0.95, eps = 1e-8, amsgrad = T)

  for (i in 1:t){
    par_r <- SGD(
      opt_target, opt,
      init_par = random_coef[[i]],
      stop_crit = epochs,
      seed = 0
      ) %>% tail()

    par_c <- SGD_CPP(
      opt_target, opt,
      init_par = random_coef[[i]],
      stop_crit = epochs,
      seed = 0
    ) %>% tail()

    expect_equal(par_c, par_r)
  }

  set.seed(NULL)
})
```

## Benchmark R vs. C++

```{r R vs cpp benchmark}
#| cache: true

set.seed(0)
data("horses")
design <- horses$Temperature %>% ExpandBspline()
opt_target_horses <- logistic_loglikelihood(
  horses$dead,
  design = design,
  lambda = 1e-5
  )
lr <- polynomial_schedule(0.1, 0.01, later = 100, p = 1)
adam <- Adam_Optimizer(lr, batch_size = floor(nrow(horses)/4))

sgd_r <- function(){
  SGD(
  opt_target_horses, adam, init_par = rep(0,opt_target_horses$n_param),
  stop_crit = 50,
  seed = 0
  ) %>% tail()
}

sgd_c <- function(){
  SGD_CPP(
    opt_target_horses, adam, init_par = rep(0,opt_target_horses$n_param),
    stop_crit = 50,
    seed = 0
  ) %>% tail()
}

microbenchmark::microbenchmark(
  sgd_c(),
  sgd_r(),
  check = "equal"
) %>% 
  ggplot2::autoplot()

```


## Benchmarking for non-linear regression {.smaller}

```{r Benchmarking}
#| cache: true
#| echo: true

decay <- polynomial_schedule(1, 0.1, later = 500)
proportional_batch <- function(f, propto = function(n) n) {
  function(epoch, n, ...){
    rep(ceiling(propto(n) * f), length(epoch))
  }
}

optimizers <- list(
  gd_vanilla = Vanilla_Optimizer(decay, proportional_batch(1)),
  sgd_vanilla = Vanilla_Optimizer(decay, proportional_batch(1, sqrt)),
  gd_momentum = Momentum_Optimizer(decay, proportional_batch(1)),
  sgd_momentum = Momentum_Optimizer(decay, proportional_batch(1, sqrt)),
  gd_adam = Adam_Optimizer(decay, proportional_batch(1)),
  sgd_adam = Adam_Optimizer(decay, proportional_batch(1, sqrt))
)

set.seed(0)
data("horses")
design_spline <- horses$Temperature %>% ExpandBspline()
design_poly <- horses$Temperature %>% ExpandBspline()
opt_spline <- logistic_loglikelihood(
  horses$dead, design = design_spline, lambda = 1e-5)
opt_poly <- logistic_loglikelihood(
  horses$dead, design = design_poly, lambda = 1e-5)

baseline_trace <- SGD_CPP(
      opt_spline, optimizers$gd_vanilla, stop_crit = 200, seed = 0)
baseline <- baseline_trace %>% tail("o")
sc <- stopping_criterion(maxiter = 5e2, threshhold_obj = baseline)

mb <- microbenchmark::microbenchmark(
  SGD_CPP(opt_spline, optimizers$gd_vanilla, stop_crit = sc, seed = 0),
  SGD_CPP(opt_poly, optimizers$gd_vanilla, stop_crit = sc, seed = 0),
  SGD_CPP(opt_spline, optimizers$sgd_vanilla, stop_crit = sc, seed = 0),
  SGD_CPP(opt_poly, optimizers$sgd_vanilla, stop_crit = sc, seed = 0),
  SGD_CPP(opt_spline, optimizers$gd_momentum, stop_crit = sc, seed = 0),
  SGD_CPP(opt_poly, optimizers$gd_momentum, stop_crit = sc, seed = 0),
  SGD_CPP(opt_spline, optimizers$sgd_momentum, stop_crit = sc, seed = 0),
  SGD_CPP(opt_poly, optimizers$sgd_momentum, stop_crit = sc, seed = 0),
  SGD_CPP(opt_spline, optimizers$gd_adam, stop_crit = sc, seed = 0),
  SGD_CPP(opt_poly, optimizers$gd_adam, stop_crit = sc, seed = 0),
  SGD_CPP(opt_spline, optimizers$sgd_adam, stop_crit = sc, seed = 0),
  SGD_CPP(opt_poly, optimizers$sgd_adam, stop_crit = sc, seed = 0),
  times = 50
)
```

## Benchmark results

```{r Display Benchmark results}
mb %>% ggplot2::autoplot()
```

## Calculating $\Omega$ for a cubic spline basis {.smaller}
To calculate the penalty matrix, we should evaluate
$$
\Omega_{ij} = \int_{-\infty}^{\infty} \varphi_i''(x)\varphi_j''(x)dx = \sum_{m = 1}^{n} \int_{k_{m-1}}^{k_m} \varphi_i''(x)\varphi_j''(x) dx := \sum_{m = 1}^{n} \int_{k_{m-1}}^{k_m} g_{ij}(x) dx
$$
Where we used that we don't care to penalize outside the boundary knots $k_0$ and $k_m$. This could for example be achieved by using natural splines.

Note that for $\varphi_i$ a 3-degree spline, $\varphi_i$ is linear between any two knots $k_{m-1}$, $k_{m}$ and thus $g_{ij}$ is a quadratic on this interval. Hence the Simpson-integration rule
$$
\int_a^b g_{ij}(x)dx = \frac{b-a}{6}\left(g_{ij}(a) + 4g_{ij}\left(\frac{a+b}{2}\right) + g_{ij}(b)\right)
$$
is an exact equality.

## Calculating $\Omega$ for a polynomial basis {.smaller}
Letting

$$
\varphi_i(x) = x^i
$$

For $i,j > 1$ we get
$$
\Omega_{ij} = \int_0^1 \varphi_i''(x)\varphi_j''(x)dx = ij(i-1)(j-1)\int_0^1 x^{i+j-4}dx = \frac{ij(i-1)(j-1)}{i+j-3}
$$

And for $i,j \leq 2$ we get $\Omega_{ij} = 0$.

## Stopping criteria{.scrollable}

```r
stopping_criterion <- function(
    maxiter = 50,
    tol_obj = NULL,
    threshhold_obj = -Inf,
    norm_obj = function(x) sum(x),
    tol_param = NULL,
    norm_param = function(x) sqrt(sum(x^2))
   ){

  # If stopping_criterion is called on a CompStatStoppingCriterion, do nothing
  if (class(maxiter) %in% c("CompStatStoppingCriterion")){
    return(maxiter)
  }

  stopper <- function(
    epoch,
    param = NULL, param_old = NULL,
    obj = NULL, obj_old = NULL){

    # Check maxiter criterion
    if (maxiter <= epoch){
      return(TRUE)
    }

    # Check criteria on parameter
    if (!is.null(param) & !is.null(param_old)){
      if (!is.null(tol_param)){
        if (norm_param(param - param_old) <= tol_param * (norm_param(param_old) + tol_param)){
          return(TRUE)
        }
      }
    }

    # Check criterion on objective
    if (!is.null(obj) & !is.null(obj_old)){
      if (!is.null(tol_obj)){
        if (norm_param(obj_old - obj) <= tol_obj * (norm_param(obj_old) + tol_obj)){
          return(TRUE)
        }
      }
    }

    # Check if objective is small enough
    if (!is.null(obj)){
      if (obj < threshhold_obj){
        return(TRUE)
      }
    }

    return(FALSE)
  }

  structure(list(
    check = stopper,
    maxiter = maxiter,
    tol_obj = tol_obj,
    tol_param = tol_param,
    threshhold_obj = threshhold_obj
    ),
    class = "CompStatStoppingCriterion"
  )
}
```



