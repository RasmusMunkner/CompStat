---
title: "Stochastic Optimization"
author: "Rasmus V. Munkner"
format: revealjs
slide-number: c/t
---

## Relevant R-Packages

```{r Loading libraries}
#| echo: true

library(CompStat) # My implementation is bundled as a package
library(testthat) # For formal testing

```


## A matrix derivation of the relevant likelihood{style="font-size: 50%"}
Consider the negative loglikelihood
$$
H(\beta) =
- \frac{1}{N}\sum_{i=1}^N y_i \log(p_i(\beta)) + (1-y_i)\log(1-p_i(\beta)) + \lambda||f_\beta''||_2^2 :=
g(p(\beta)) + \lambda||f_\beta''||_2^2
$$
with $g: \mathbb{R}^N \rightarrow \mathbb{R}$ and $p: \mathbb{R}^p \rightarrow \mathbb{R}^n$. It holds that
$$
D_\beta H(\beta) = D_p(g(p(\beta))) \cdot D_\beta p(\beta) + \lambda D_\beta||f_\beta''||_2^2
$$

Note that
$$
p(\beta) = \frac{e^{X\beta}}{1+e^{X\beta}} \\
D_\beta p(\beta) = \text{diag}\left(\frac{e^{X\beta}}{(1+e^{X\beta})^2}\right)X
$$

And
$$
g(p) = -\frac{1}{N}\textbf{1}^T(y \log(p) + (1-y)\log(1-p)) \\
D_pg(p) = -\frac{1}{N}\textbf{1}^T\cdot\text{diag}\left(\frac{y}{p} - \frac{1-y}{1-p}\right)
$$

## Penalization {style="font-size: 50%"}
For the penalization term
$$
||f''_\beta||_2^2 =
\int f''_\beta(x)^2dx =
\int \left(\sum_{k=1}^p \varphi_k''(x)\beta_k\right)^2 dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j\int \varphi_k(x)''\varphi_j(x)'' dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j \langle\varphi_k'',\varphi_j''\rangle :=
\beta^T \Omega \beta
$$

Thus

$$
D_\beta ||f_\beta''||_2^2 = 2 \beta^T\Omega
$$

This can be seem by utilizing

$$
\beta^T\Omega \beta = \beta^T \Omega^{T/2} \Omega^{1/2} \beta = \xi^T\xi \\
D_\beta(\beta^T\Omega\beta) = D_\xi(\xi^T\xi) \cdot D_\beta(\Omega^{1/2}\beta) = 2\xi^T\Omega^{1/2} = 2\beta^T\Omega^{T/2}\Omega^{1/2} = 2\beta^T\Omega
$$

## Implementation

```r{code-line-numbers="1-6|8-13|48-55|15-20|22-33|34-46"}
logistic_loglikelihood <- function(
    response,
    design,
    penalty_matrix = NULL,
    lambda = 0.001
){

  if (c("CompStatBasisExpansion") %in% class(design)){
    penalty_matrix <- design$Omega
    design <- design$X
  } else if (is.null(penalty_matrix)){
    penalty_matrix <- diagmat(rep(1,ncol(design))) # Ordinary L2 Penalty
  }

  loglikelihood <- function(coef, batch = 1:nrow(design)){
    eta <- exp(design[batch,] %*% coef)
    p <- eta / (1 + eta)
    -mean(response[batch] * log(p) + (1-response[batch]) * log(1-p)) +
      lambda * t(coef) %*% penalty_matrix %*% coef
  }

  grad <- function(coef, batch = 1:nrow(design)){
    X <- design[batch,]
    y <- response[batch]

    eta <- exp(X %*% coef)
    p <- eta / (1 + eta)
    dp <- t(X) %*% diagmat(eta / (1 + eta)^2)
    dg <-  - (y/p - (1-y)/(1-p)) / length(batch)
    grad <- dp %*% dg %>% as.vector() + (2 * lambda * penalty_matrix %*% coef)
    grad %>% as.vector()
  }

  structure(list(
    objective = loglikelihood,
    grad = grad,
    n_param = nrow(penalty_matrix),
    n_index = length(response),
    design = design,
    response = response,
    penalty_matrix = penalty_matrix,
    lambda = lambda
  ),
  class = c("CompStatLogisticLogLikelihood", "CompStatOptimizable")
  )
}

CompStatBasisExpansion <- function(X, x, Omega, expand_new){
  structure(list(
    X = X,
    x = x,
    Omega = Omega,
    expand_new = expand_new
  ), class = "CompStatBasisExpansion")
}

#---
```


## Calculating $\Omega$ for a cubic spline basis {.smaller}
To calculate the penalty matrix, we should evaluate
$$
\Omega_{ij} = \int_{-\infty}^{\infty} \varphi_i''(x)\varphi_j''(x)dx = \sum_{m = 1}^{n} \int_{k_{m-1}}^{k_m} \varphi_i''(x)\varphi_j''(x) dx := \sum_{m = 1}^{n} \int_{k_{m-1}}^{k_m} g_{ij}(x) dx
$$
Where we used that we don't care to penalize outside the boundary knots $k_0$ and $k_m$. This could for example be achieved by using natural splines.

Note that for $\varphi_i$ a 3-degree spline, $\varphi_i$ is linear between any two knots $k_{m-1}$, $k_{m}$ and thus $g_{ij}$ is a quadratic on this interval. Hence the Simpson-integration rule
$$
\int_a^b g_{ij}(x)dx = \frac{b-a}{6}\left(g_{ij}(a) + 4g_{ij}\left(\frac{a+b}{2}\right) + g_{ij}(b)\right)
$$
is an exact equality.

## Calculating $\Omega$ for a polynomial basis {.smaller}
Letting

$$
\varphi_i(x) = x^i
$$

For $i,j > 1$ we get
$$
\Omega_{ij} = \int_0^1 \varphi_i''(x)\varphi_j''(x)dx = ij(i-1)(j-1)\int_0^1 x^{i+j-4}dx = \frac{ij(i-1)(j-1)}{i+j-3}
$$

And for $i,j \leq 2$ we get $\Omega_{ij} = 0$.


## Stochastic Optimization Algorithms
We consider three different stochastic gradient descent algorithms, each of
which share the 'Adam' interface

```r{code-line-numbers="1-3|5-14|16-19|21-22|32-47|24-26|28-30"}
Adam_Optimizer <- function(
    lr = 1e-3, beta_1 = 0.95, beta_2 = 0.97, eps = 1e-8, amsgrad = T
    ){

  update_param <- function(grad){
    rho <<- beta_1 * rho + (1-beta_1) * grad
    nu_proposal <- beta_2 * nu + (1-beta_2) * grad^2
    if (amsgrad){
      nu <<- pmax(nu_proposal, nu)
    } else {
      nu <<- nu_proposal
    }
    rho / (sqrt(nu) + eps)
  }

  reset <- function(){
    rho <<- 0
    nu <<- 0
  }

  CompStatOptimizer(lr, update_param, reset)
}

Momentum_Optimizer <- function(lr = 1e-3, beta_1 = 0.95){
  Adam_Optimizer(lr, beta_1 = beta_1, beta_2 = 1, eps = 1)
}

Vanilla_Optimizer <- function(lr = 1e-3){
  Adam_Optimizer(lr, beta_1 = 0, beta_2 = 1, eps = 1)
}

CompStatOptimizer <- function(lr, update_param, reset){

  # Ensure the learning rate is callable
  if (is.function(lr)){
    lrate <- lr
  } else {
    lrate <- function(epoch) {lr[min(epoch, length(lr))]}
  }

  structure(list(
    lr = lrate,
    update_param = update_param,
    reset = reset
  ),
  class = "CompStatOptimizer")
}

```

## Assembling a SGD algorithm

```r{code-line-numbers="1-11|13-14|16-19|21-32|34-39|41-47|49-57|60-72|74-86|90-102|106-107"}
SGD <- function(
    optimizable,
    optimizer = Vanilla_Optimizer(),
    init_param = NULL,
    stop_crit = 50,
    shuffle = T,
    batch_size = 1,
    trace_precision = "batch",
    seed = NULL,
    ...
    ){

  dqrng::dqRNGkind("Xoroshiro128+")
  dqrng::dqset.seed(seed)

  # Ensure stopping criterion is valid
  if (!(class(stop_crit) %in% c("CompStatStoppingCriterion"))){
    stop_crit <- stopping_criterion(maxiter = stop_crit)
  }

  # Determine optimizer
  if (!(class(optimizer) %in% c("CompStatOptimizer"))){
    opt <-
      switch(optimizer,
           "vanilla"= Vanilla_Optimizer(...),
           "momentum"= Momentum_Optimizer(...),
           "adam" = Adam_Optimizer(...)
           )
  } else {
    opt <- optimizer
  }
  opt$reset()

  # Useful control quantities
  batches_per_epoch <- ceiling(optimizable$n_index / batch_size)
  max_total_updates <- batches_per_epoch * stop_crit$maxiter

  # Initialize parameters
  trace <- CompStatTrace(optimizable)

  if (is.null(init_param)){
    init_param <- rep(NA, optimizable$n_param)
  }
  init_param <- init_param %>% dplyr::coalesce(rnorm(optimizable$n_param))
  par_next <- init_param

  trace <- extend(trace, init_param)

  for (epoch in 1:stop_crit$maxiter){

    # Reshuffle observations
    if (shuffle){
      index_permutation <- dqrng::dqsample.int(
        optimizable$n_index, optimizable$n_index, replace = F)
    } else {
      index_permutation <- 1:optimizable$n_index
    }

    # Apply minibatch gradient updates
    for (b in 1:batches_per_epoch){

      par_now <- par_next

      grad <- optimizable$grad(
        par_now,
        index_permutation[(1+(b-1)*batch_size):
                            min(b*batch_size, optimizable$n_index)]
      )

      update <- opt$lr(epoch) * opt$update_param(grad)

      par_next <- par_now - update

      if (trace_precision == "batch"){
        trace <- trace %>% extend(par_next)
        if (stop_crit$check(
          epoch,
          param = trace %>% tail(1),
          param_old = trace %>% tail(2),
          obj = trace %>% tail(1, type = "o"),
          obj_old = trace %>% tail(2, type = "o")
          )
        ){
          return(trace)
        }
      }

    }

    if (trace_precision == "epoch"){
      trace <- trace %>% extend(par_next)
      if (stop_crit$check(
        epoch,
        param = trace %>% tail(1),
        param_old = trace %>% tail(2),
        obj = trace %>% tail(1, type = "o"),
        obj_old = trace %>% tail(2, type = "o")
        )
      ){
        return(trace)
      }
    }

  }

  trace <- trace %>% extend(par_next)
  return(trace)
}
```

## Checking that everything works

```{r Checking that SGD works}
#| echo: true
#| code-line-numbers: "2-7|8-19|20-22"
test_that("sgd converges to the right values ", {
  set.seed(0)
  targets <- rnorm(10)
  init_param <- rnorm(10)
  sc <- stopping_criterion(maxiter = 250)
  lr <- polynomial_schedule(0.4, 0.05, later=100, p=1)
  opt_target <- optimizable_parabola(targets)
  trace_vanilla <- SGD(
    opt_target, Vanilla_Optimizer(lr),
    init_param = init_param, stop_crit = sc
    )
  trace_momentum <- SGD(
    opt_target, Momentum_Optimizer(lr, beta_1 = 0.8),
    init_param = init_param, stop_crit = sc
  )
  trace_adam <- SGD(
    opt_target, Adam_Optimizer(lr, beta_1 = 0.8),
    init_param = init_param, stop_crit = sc
  )
  expect_equal(max(abs(tail(trace_vanilla, 1) - targets)) < 1e-6, TRUE)
  expect_equal(max(abs(tail(trace_momentum, 1) - targets)) < 1e-6, TRUE)
  expect_equal(max(abs(tail(trace_adam, 1) - targets)) < 1e-6, TRUE)

  set.seed(NULL)
})
```

## Visualizing the results

```{r Checking that SDG works with plots}
set.seed(0)
targets <- rnorm(10)
init_par <- rnorm(10)
sc <- stopping_criterion(maxiter = 100)
lr <- polynomial_schedule(0.4, 0.05, later=100, p=1)
opt_target <- optimizable_parabola(targets)
trace_vanilla <- SGD(
  opt_target, Vanilla_Optimizer(lr),
  init_par = init_par, stop_crit = sc
  )
trace_momentum <- SGD(
  opt_target, Momentum_Optimizer(lr, beta_1 = 0.8),
  init_par = init_par, stop_crit = sc
)
trace_adam <- SGD(
  opt_target, Adam_Optimizer(lr, beta_1 = 0.8),
  init_par = init_par, stop_crit = sc
)
  
p1 <- trace_vanilla %>% plot("p") + ggplot2::ggtitle("Vanilla")
p2 <- trace_momentum %>% plot("p") + ggplot2::ggtitle("Momentum")
p3 <- trace_adam %>% plot("p") + ggplot2::ggtitle("Adam")

gridExtra::grid.arrange(grobs = list(p1, p2, p3), nrow = 2)

set.seed(NULL)
```

## Applying SGD to the horse data

```{r Applying SGD to horse data}
#| cache: true
set.seed(0)
data("horses")
design <- horses$Temperature %>% ExpandBspline()
opt_target_horses <- logistic_loglikelihood(
  horses$dead,
  design = design,
  lambda = 1e-5
  )
lr <- polynomial_schedule(0.1, 0.01, later = 100, p = 1)
adam <- Adam_Optimizer(lr, batch_size = floor(nrow(horses)/4))
trace <- SGD(
  opt_target_horses, adam, init_par = rep(0,opt_target$n_param),
  stop_crit = 200,
  seed = 0
  )
set.seed(NULL)

p1 <- plot(trace, "p") + ggplot2::ggtitle("Parameter Values")

p2 <- plot(trace, "o") + ggplot2::ggtitle("Objective Function")

p3 <- plot(design, trace %>% tail(1), post_transform = function(x) exp(x) / (1 + exp(x))) +
  ggplot2::lims(x = c(34,42), y = c(0,1)) +
  ggplot2::labs(x = "Temperature", y = "P(Dead|Temperature)") +
  ggplot2::ggtitle("Final Fitted Regression")

p4 <- ggplot2::ggplot(horses, ggplot2::aes(x = Temperature, y = dead)) +
  ggplot2::geom_smooth(method = "loess", se = F) +
  ggplot2::lims(x = c(34,42), y = c(0,1)) +
  ggplot2::labs(x = "Temperature", y = "Loess smoother (Dead | Temperature)") +
  ggplot2::ggtitle("Loess smoother")

gridExtra::grid.arrange(grobs = list(p1, p2, p3, p4), nrow = 2)

```

## Garbage collection turns out to be the bottleneck{.smaller}

```{r Profiling r SGD}
#| cache: true

set.seed(0)
profvis::profvis({
  trace <- SGD(
  opt_target_horses, adam, init_par = rep(0,opt_target$n_param),
  stop_crit = 1000
  )
})
set.seed(NULL)

```

## C++ implementation

```cpp{code-line-numbers="1-8|9-17|19-20|22-26|28-34|36-46|48-49|51-52"}
Rcpp::List SGD_CPP_PRIMITIVE(
    const arma::mat &design, arma::vec coef, const arma::vec &y,
    const arma::mat &pen_matrix, const double &lambda,
    const NumericVector &lr, const int &maxiter, int &batch_size,
    const double &adam_beta1, const double &adam_beta2, const double &adam_eps,
    const bool &amsgrad,
    const int &seed
){
  Rcpp::List coef_list (maxiter);
  arma::vec grad;
  arma::vec rho = vec(coef.n_elem); rho.zeros();
  arma::vec nu = vec(coef.n_elem); nu.zeros();
  batch_size = std::min<int>(y.n_elem, batch_size);
  int batches_per_epoch = std::ceil(double(y.n_elem) / batch_size);
  uvec indicies_all, indicies;
  dqrng::dqRNGkind("Xoroshiro128+");
  dqrng::dqset_seed(IntegerVector::create(seed)); // Sets the shuffling seed

  for (int i = 0; i < maxiter; i++){
    indicies_all = as<uvec>(dqrng::dqsample_int(y.n_elem, y.n_elem));

    for (int b = 0; b < batches_per_epoch; b++){
      indicies = indicies_all.subvec(
        b * batch_size,
        std::min<int>((b+1) * batch_size - 1, y.n_elem - 1)
      );

      grad = lll_gradC(
        design.rows(indicies),
        coef,
        y.elem(indicies),
        pen_matrix,
        lambda
        );

      rho = rho * adam_beta1 + grad * (1 - adam_beta1);
      if (amsgrad){
        nu = arma::max(
          nu * adam_beta2 + pow(grad, 2) * (1 - adam_beta2), nu
        );
      } else {
        nu = nu * adam_beta2 + pow(grad, 2) * (1 - adam_beta2);
      }

      coef = coef - lr[i] * rho / (sqrt(nu) + adam_eps);
    }

    coef_list[i] = as<NumericVector>(wrap(coef)); // Breaks mutability of coef
  }

  return(coef_list);
}
```

## Testing consistency

```{r Testing consistency across R and c++}
#| cache: true
#| echo: true
#| code-line-numbers: "3-5|7-14|16-22|24-39"
test_that("C++ sgd converges to the right value", {

  set.seed(0)
  t <- 10
  p <- 8

  targets <- rnorm(p)
  sll <- simple_logistic_loglikelihood(n = 647, p = p, beta = targets)

  opt_target <- logistic_loglikelihood(
    design = sll$X, response = sll$y,
    penalty_matrix = matrix(0, nrow = ncol(sll$X), ncol = ncol(sll$X)),
    lambda = 0
  )

  random_coef <- 1:t %>% purrr::map(.f = function(i) rnorm(p))

  epochs <- 50
  batch_size <- 24
  lr <- polynomial_schedule(0.1, 0.001, 50)
  opt <- Adam_Optimizer(
    lr, beta_1 = 0.9, beta_2 = 0.95, eps = 1e-8, amsgrad = T)

  for (i in 1:t){
    par_r <- SGD(
      opt_target, opt, init_param = random_coef[[i]],
      stop_crit = epochs, batch_size = batch_size, trace_precision = "none",
      seed = 0
      ) %>% tail(1)

    par_c <- SGD_CPP(
      lll = opt_target,
      init_coef = random_coef[[i]],
      lr = lr, beta1 = 0.9, beta2 = 0.95, eps = 1e-8, batch_size = batch_size,
      stop_crit = epochs, amsgrad = T,
      seed = 0
    ) %>% tail(1) %>% unlist()

    expect_equal(par_c, par_r)
  }

  set.seed(NULL)
})
```

## Benchmark R vs. C++

```{r R vs cpp benchmark}
#| cache: true

set.seed(0)
data("horses")
design <- horses$Temperature %>% ExpandBspline()
opt_target_horses <- logistic_loglikelihood(
  horses$dead,
  design = design,
  lambda = 1e-5
  )
lr <- polynomial_schedule(0.1, 0.01, later = 100, p = 1)
adam <- Adam_Optimizer(lr, batch_size = floor(nrow(horses)/4))

sgd_r <- function(){
  SGD(
  opt_target_horses, adam, init_par = rep(0,opt_target$n_param),
  stop_crit = 50,
  seed = 0
  ) %>% tail()
}

sgd_c <- function(){
  SGD_CPP(
    opt_target_horses, adam, init_par = rep(0,opt_target$n_param),
    stop_crit = 50,
    seed = 0
  ) %>% tail()
}

microbenchmark::microbenchmark(
  sgd_c(),
  sgd_r(),
  check = "equal"
) %>% 
  ggplot2::autoplot()

```


## Benchmarking for non-linear regression

```{r Setting up benchmarks cases}

set.seed(3)
reg <- univariate_logistic_regression(n = 1)
plot(reg)

n <- 1e6
x <- runif(n, -1, 1)
p <- reg$f_true(x)
y <- rbinom(n, 1, p)


nval <- c(7, 11, 15, 19)
targets <- nval %>% purrr::map(.f = function(i){
  x_spline <- x[1:2^i] %>% ExpandBspline()
  x_poly <- x[1:2^i] %>% ExpandPoly()
  list(
    x_spline = x_spline,
    x_poly = x_poly,
    lll_spline = logistic_loglikelihood(y[1:2^i], x_spline, lambda = 1e-5),
    lll_poly = logistic_loglikelihood(y[1:2^i], x_poly, lambda = 1e-5)
  )
}) %>% magrittr::set_names(paste0("n", 2^nval))

set.seed(NULL)

```

```{r Benchmarking}

decay_fast <- polynomial_schedule(0.1, 0.9, later = 1)
decay_slow <- polynomial_schedule(0.1, 0.9, later = 1, p = 0.5)
proportional_batch <- function(f, propto = function(n) n) {
  function(epoch, n){
    rep(ceil(propto(n) * f), length(epoch))
  }
}

optimizers <- list(
  gd_vanilla_fast = Vanilla_Optimizer(decay_fast, proportional_batch(1)),
  sgd_vanilla_fast = Vanilla_Optimizer(decay_fast, proportional_batch(1, sqrt)),
  gd_vanilla_slow = Vanilla_Optimizer(decay_slow, proportional_batch(1)),
  sgd_vanilla_slow = Vanilla_Optimizer(decay_slow, proportional_batch(1, sqrt)),
  gd_adam_fast = Adam_Optimizer(decay_fast, proportional_batch(1)),
  sgd_adam_fast = Adam_Optimizer(decay_fast, proportional_batch(1, sqrt)),
  gd_adam_slow = Adam_Optimizer(decay_slow, proportional_batch(1)),
  sgd_adam_slow = Adam_Optimizer(decay_slow, proportional_batch(1, sqrt))
)

lr <- polynomial_schedule(1, 0.1, later = 300)
adam <- Vanilla_Optimizer(lr)
trace1 <- SGD_CPP(
  targets[[1]]$lll_spline, adam,
  seed = 0, stop_crit = 300, batch_size = 100
  )
trace2 <- SGD_CPP(
  targets[[1]]$lll_spline, adam,
  seed = 1, stop_crit = 300, batch_size = 50
  )

CompareTraces(list(LargeBatch = trace1, SmallBatch = trace2, Dup1 = trace1, Dupe2 = trace2), "po")

plot(trace)

plot(targets[[1]]$x_spline, trace %>% tail("p"))

trace %>% 
  dplyr::select(obj) %>% 
  dplyr::mutate(iter = dplyr::row_number()) %>% 
  ggplot2::ggplot(ggplot2::aes(x = iter, y = obj)) +
  ggplot2::geom_line()

trace %>% 
  dplyr::mutate(iter = dplyr::row_number()) %>% 
  tidyr::pivot_longer(cols = -iter, names_to = "parameter", values_to = "value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = iter, y = value, color = parameter)) +
  ggplot2::geom_line()

plot(targets[[1]]$x_spline, trace %>% tail(1) %>% unlist(), post_transform = function(x) exp(x) / (1 + exp(x)))

ggplot2::ggplot(mapping = ggplot2::aes(x = x[1:32768], y = y[1:32768])) +
  ggplot2::geom_smooth(method = "gam")

```





## TODO
- Show off the R implementation of SGD and logistic likelihood - Yes
- Show off the OO side of things (focus on basis expansions) - Yes
- Profile the R implementation - Yes
- Show off the C++ implementation - Yes
- Show off the unit tests - Yes
- Benchmark the C++ implementation


