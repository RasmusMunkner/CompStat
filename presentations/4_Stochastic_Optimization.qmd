---
title: "Stochastic Optimization"
author: "Rasmus V. Munkner"
format: revealjs
---

## Problem statement
We consider the issue of minimizing a penalized, negative loglikelihhood

$$
H(\beta) = -\frac{1}{N}\sum_{i=1}^N\left(y_i\log p_i(\beta) + (1-y_i)\log(1-p_i(\beta))\right) + \lambda||f_\beta''||_2^2
$$
where
$$
\text{logit}(p_i(\beta)) = f(x_i\mid \beta) = \varphi(x_i)^T\beta
$$

## Some mathematical details {style="font-size: 35%"}
Note that
$$
||f''_\beta||_2^2 =
\int f''_\beta(x)^2dx =
\int \left(\sum_{k=1}^p \varphi_k''(x)\beta_k\right)^2 dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j\int \varphi_k(x)''\varphi_j(x)'' dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j \langle\varphi_k'',\varphi_j''\rangle :=
\beta^T \Omega \beta
$$

Also
$$
\log(p_i(\beta)) =
\log\left(\frac{e^{\varphi(x_i)^T\beta}}{1+e^{\varphi(x_i)^T\beta}}\right) =
\varphi_i(x)^T\beta - \log\left(1 + e^{\varphi(x_i)^T\beta}\right) \\
\log(1-p_i(\beta)) =
\log\left(1 - \frac{e^{\varphi(x_i)^T\beta}}{1+e^{\varphi(x_i)^T\beta}}\right) =
-\log\left(1 + e^{\varphi(x_i)^T\beta}\right)
$$

Hence
$$
\nabla_\beta \log(p_i(\beta)) =
\varphi(x_i) - \frac{e^{\varphi(x_i)^T\beta}}{1 + e^{\varphi(x_i)^T\beta}}\varphi(x_i) =
\frac{1}{1 + e^{\varphi(x_i)^T\beta}}\varphi(x_i) \\
\nabla_\beta \log(1 - p_i(\beta)) =
-\frac{e^{\varphi(x_i)^T\beta}}{1 + e^{\varphi(x_i)^T\beta}}\varphi(x_i)
$$

Thus
$$
\nabla_\beta H(\beta) =
-\frac{1}{N}\sum_{i=1}^N\left( \frac{y_i}{1 + e^{\varphi(x_i)^T\beta}}\varphi(x_i) - \frac{(1-y_i)e^{\varphi(x_i)^T\beta}}{1 + e^{\varphi(x_i)^T\beta}}\varphi(x_i)\right) + 2\lambda \Omega\beta := \\
-\frac{1}{N}\sum_{i=1}^N \left(\frac{y_i + (1-y_i)\eta_i}{1+\eta_i}\right)\varphi(x_i) + 2\lambda \Omega \beta =
-\frac{1}{N}\sum_{i=1}^N \left(\frac{\eta_i}{1 + \eta_i} + y_i\frac{1-\eta_i}{1+\eta_i}\right)\varphi(x_i) + 2\lambda \Omega \beta
$$













