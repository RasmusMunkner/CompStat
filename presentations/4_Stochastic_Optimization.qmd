---
title: "Stochastic Optimization"
author: "Rasmus V. Munkner"
format: revealjs
slide-number: c/t
---

## Problem statement
We consider the issue of minimizing a penalized, negative loglikelihood

$$
H(\beta) = -\frac{1}{N}\sum_{i=1}^N\left(y_i\log p_i(\beta) + (1-y_i)\log(1-p_i(\beta))\right) + \lambda||f_\beta''||_2^2
$$
where
$$
\text{logit}(p_i(\beta)) = f(x_i\mid \beta) = \varphi(x_i)^T\beta
$$


## A matrix derivation of the relevant likelihood{style="font-size: 35%"}
Consider the negative loglikelihood without penalization
$$
H(\beta) =
- \frac{1}{N}\sum_{i=1}^N y_i \log(p_i(\beta)) + (1-y_i)\log(1-p_i(\beta)) =
g(p(\beta))
$$
with $g: \mathbb{R}^N \rightarrow \mathbb{R}$ and $p: \mathbb{R}^p \rightarrow \mathbb{R}^n$. It holds that
$$
D_\beta H(\beta) = D_p(g(p(\beta))) \cdot D_\beta p(\beta)
$$

Note that
$$
p(\beta) = \frac{e^{X\beta}}{1+e^{X\beta}}
$$
Thus
$$
D_\beta p(\beta) = \text{diag}\left(\frac{e^{X\beta}}{(1+e^{X\beta})^2}\right)X
$$

And
$$
g(p) = -\frac{1}{N}\textbf{1}^T(y \log(p) + (1-y)\log(1-p))
$$

Thus
$$
D_pg(p) = -\frac{1}{N}\textbf{1}^T\cdot\text{diag}\left(\frac{y}{p} - \frac{1-y}{1-p}\right)
$$



## Penalization {style="font-size: 35%"}
For the penalization term
$$
||f''_\beta||_2^2 =
\int f''_\beta(x)^2dx =
\int \left(\sum_{k=1}^p \varphi_k''(x)\beta_k\right)^2 dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j\int \varphi_k(x)''\varphi_j(x)'' dx =
\sum_{k = 1}^p \sum_{j = 1}^p \beta_k \beta_j \langle\varphi_k'',\varphi_j''\rangle :=
\beta^T \Omega \beta
$$

Thus

$$
D_\beta ||f_\beta''||_2^2 = 2 \beta^T\Omega
$$

This can be seem by utilizing

$$
\beta^T\Omega \beta = \beta^T \Omega^{T/2} \Omega^{1/2} \beta = \xi^T\xi \\
D_\beta(\beta^T\Omega\beta) = D_\xi(\xi^T\xi) \cdot D_\beta(\Omega^{1/2}\beta) = 2\xi^T\Omega^{1/2} = 2\beta^T\Omega^{T/2}\Omega^{1/2} = 2\beta^T\Omega
$$

## On the calculation of $\Omega$ {.smaller}
To calculate the penalty matrix, we should evaluate
$$
\Omega_{ij} = \int_{-\infty}^{\infty} \varphi_i''(x)\varphi_j''(x)dx = \sum_{m = 1}^{n} \int_{k_{m-1}}^{k_m} \varphi_i''(x)\varphi_j''(x) dx := \sum_{m = 1}^{n} \int_{k_{m-1}}^{k_m} g_{ij}(x) dx
$$
Where we used that we don't care to penalize outside the boundary knots $k_0$ and $k_m$. This could for example be achieved by using natural splines.

Note that for $\varphi_i$ a 3-degree spline, $\varphi_i$ is linear between any two knots $k_{m-1}$, $k_{m}$ and thus $g_{ij}$ is a quadratic on this interval. Hence the Simpson-integration rule
$$
\int_a^b g_{ij}(x)dx = \frac{b-a}{6}\left(g_{ij}(a) + 4g_{ij}\left(\frac{a+b}{2}\right) + g_{ij}(b)\right)
$$
is an exact equality.

## Stochastic Optimization Algorithms
We consider versions of stochastic gradient descent with updates
$$
\beta^{(n+1)} = \beta^{(n)} - \rho^{(n)} \nabla H(\beta^{(n)})
$$
And corresponding version with minibatches, momentum and Adam.

## Benchmark

```{r Testing SGD Implementations, eval = F}

logistic_ll <- make_logistic_loglikelihood()

lr <- polynomial_schedule(0.1, 0.001, K = 50)
vanilla_sgd_opt <- Vanilla_Optimizer(lr)
momentum_sgd_opt <- Adam_Optimizer(lr, beta_1 = 0.9, beta_2 = 1, eps = 1)
adam_sgd_opt <- Adam_Optimizer(lr, beta_1 = 0.9, beta_2 = 0.99)

set.seed(0)
opt_param <- rnorm(10)
init_param <- rnorm(10)
test_optimization <- optimizable_parabola(opt_param)
set.seed(NULL)

sc <- stopping_criterion(tol_obj = 1e-6, maxiter = 999)

trace_vanilla <- SGD(
  test_optimization,
  optimizer = vanilla_sgd_opt,
  init_param = init_param,
  stop_crit = sc
)

trace_momentum <- SGD(
  test_optimization,
  optimizer = momentum_sgd_opt,
  init_param = init_param,
  stop_crit = sc
)

trace_adam <- SGD(
  test_optimization,
  optimizer = adam_sgd_opt,
  init_param = init_param,
  stop_crit = sc
)

trace_vanish <- SGD(
  test_optimization,
  optimizer = vanish_sgd_opt,
  init_param = init_param,
  stop_crit = sc
)

trace_vanilla %>% plot()
trace_momentum %>% plot()
trace_adam %>% plot()
trace_vanish %>% plot()

final_params <- list(trace_vanilla, trace_momentum, trace_adam) %>% 
  purrr::map_dfr(.f = tail)

final_params %>% 
  tibble::add_row(opt_param %>% as.list() %>% magrittr::set_names(colnames(final_params)) %>% tibble::as_tibble()) %>% 
  purrr::map_dfc(.f = function(x){x %>% range() %>% diff()})

```

```{r Profiling Full R Implementation, eval = F}

logistic_opfun <- make_logistic_loglikelihood()
adam_sgd_opt <- Adam_Optimizer(1e-3)

trace <- SGD(logistic_opfun, adam_sgd_opt, batch_size = 100, stop_crit = 200)
plot(trace)

profvis::profvis(
  SGD(
    logistic_opfun, adam_sgd_opt, batch_size = 100,
    stop_crit = 1000, trace_precision ="epoch"),
  simplify = F
)

microbenchmark::microbenchmark(
  SGD(logistic_opfun, vanilla_sgd_opt, batch_size = 100, stop_crit = 10, trace_precision = "batch"),
  SGD(logistic_opfun, vanilla_sgd_opt, batch_size = 100, stop_crit = 10, trace_precision = "epoch"),
  SGD(logistic_opfun, vanilla_sgd_opt, batch_size = 100, stop_crit = 10, trace_precision = "none")
)

```


```{r Microbenchmarks, eval = F}

sgd_vanilla <- function(){
  SGD(
    logistic_opfun,
    optimizer = vanilla_sgd_opt,
    stop_crit = sc
  )
}

sgd_vanilla()

sgd_momentum <- function(){
  SGD(
    logistic_opfun,
    optimizer = momentum_sgd_opt,
    init_param = init_param,
    stop_crit = sc
  )
}

sgd_adam <- function(){
  SGD(
    logistic_opfun,
    optimizer = adam_sgd_opt,
    init_param = init_param,
    stop_crit = sc
  )
}

profvis::profvis(
  sgd_vanilla()
)


microbenchmark::microbenchmark(
  sgd_vanilla(),
  sgd_momentum(),
  sgd_adam()
) %>% 
  ggplot2::autoplot()


```












