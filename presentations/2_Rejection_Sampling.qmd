---
title: "Rejection Sampling"
author: "Rasmus V. Munkner"
format: revealjs
slide-number: c/t
---

## Problem statement

- Want to sample i.i.d. observations $(X_n)_{n \in \mathbb{N}} \sim f_0$.

- We only know $f_0$ up to a normalizing constant.

. . .

Steps:

- Find an envelope $g$ such that $\alpha f_0 \leq g$.

. . .

- Sample observations $(Y_n)_{n \in \mathbb{N}} \sim g$.

. . .

- Accept/reject observations based on $U_n \leq \alpha \frac{g(Y_n)}{f_0(Y_n)} := \frac{\tilde{g}(Y_n)}{\tilde{f}(Y_n)}$

## RandomVariable

```{r Demonstrating RandomVariable Gaussian}
#| echo: true
library(CompStat)
X <- named_rv("normal")
print(X)
plot(X)
```

## RandomVariable

```{r Demonstrating RandomVariable Poisson Prior}
#| echo: true
Y <- named_rv("poisson prior")
print(Y)
plot(Y)
```

## Envelope

```{r Demonstrating Envelope Laplace}
#| echo: true
epa_rv <- named_rv("epanechnikov")
lap_enve <- LaplaceEnvelope(epa_rv)
print(lap_enve)
plot(lap_enve)
```

## Envelope

```{r Demonstrating Envelope Gaussian}
#| echo: true

epa_rv <- named_rv("epanechnikov")
gauss_enve <- GaussianEnvelope(epa_rv)
print(gauss_enve)
plot(gauss_enve)
```

## Envelope

```{r Demonstrating Envelope LogLinear}
#| echo: true

epa_rv <- named_rv("epanechnikov")
loglinear_enve <-
  LogLinearEnvelope(
    epa_rv,
    tangent_points = c(-0.7, -0.1, 0.2, 0.4, 0.8)
    )
print(loglinear_enve)
plot(loglinear_enve)


```
## Sampler

```{r Demonstrating rejection_sampler}
#| echo: true
rv_epa <- named_rv("epa")
enve <- LogLinearEnvelope(rv_epa, autoselection_msg = F)
sampler <- rejection_sampler(enve)
set.seed(0)
sim <- sampler(100000)
set.seed(NULL)
```

```{r Plotting rejection sampler results}
x <- seq(-3,3,0.01)
y <- ifelse(abs(x)<1, 3/4 * (1-x^2), 0)
both <- ggplot2::ggplot() +
  ggplot2::geom_histogram(ggplot2::aes(x = sim, ggplot2::after_stat(density)), bins = 75) +
  ggplot2::geom_line(ggplot2::aes(x = x, y = y), color = "red") +
  ggplot2::labs(x = "x", y = "Density") +
  ggplot2::ggtitle("Both")
true_dens <- ggplot2::ggplot() +
  ggplot2::geom_line(ggplot2::aes(x = x, y = y), color = "red") +
  ggplot2::labs(x = "x", y = "Density") +
  ggplot2::ggtitle("True density")
sim_hist <- ggplot2::ggplot() +
  ggplot2::geom_histogram(ggplot2::aes(x = sim, ggplot2::after_stat(density)), bins = 75) +
  ggplot2::labs(x = "x", y = "Density") +
  ggplot2::ggtitle("Simulated data")
gridExtra::grid.arrange(grobs = list(sim_hist, true_dens, both), nrow = 2, ncol = 2)
set.seed(NULL)

```

## Epanechnikov Benchmark

```{r Initial Benchmark for envelopes (epanechnikov)}
#| cache: true
bm <- benchmark_envelopes(rv = named_rv("epanechnikov"))
bm %>%
  ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
```

## Poisson Prior Benchmark

```{r Initial Benchmark for envelopes (poisson prior)}
#| cache: true
suppressWarnings(
  bm <- benchmark_envelopes(N_seq = 4:8, rv = named_rv("poisson prior"))
)
bm %>%
  ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
```

## Profiling the fast implementations{.smaller .scrollable}

```{r Profiling Samplers, eval = F}
#| cache: true
#| echo: true

set.seed(0)

suppressWarnings(
  gauss_sampler <- named_rv("poisson prior") %>% GaussianEnvelope() %>% rejection_sampler()
)
adapted_sampler <- named_rv("poisson prior") %>%
  LogLinearEnvelope(autoselection_msg = F) %>%
  rejection_sampler(evalmode = 1)

for (i in 1:50){
  adapted_sampler(10000, train = T, adapt_enve = T)
}

profvis::profvis(
  adapted_sampler(300000),
  simplify = F
)

profvis::profvis(
  gauss_sampler(300000),
  simplify = F
)

set.seed(NULL)

```

## Profiling the fast implementations{.smaller}

![Profvis for adaptive envelope](2_poorly_rendered_profile_adapt.png)


![Profvis for gaussian envelope](2_poorly_rendered_profile_gauss.png)

- Turns out that the exponential function is the time-consuming part!

## Revisiting the poisson prior{.smaller}
The target log-density is
$$
\log(f_0(y)) = \sum_{i=1}^n yz_ix_i - e^{yx_i} = y\left(\sum_{i=1}^{100} z_ix_i\right) - \sum_{i=1}^{100} e^{yx_i} 
$$

. . .

- Due to the second term, exponentiation has to be performed 100 times.

- We suggest an approximation based on a Taylor-expansion.

## Taylor expansion{.smaller}
We have
$$
\sum_{i=1}^{100}e^{yx_i} =
e^{\bar{x}y}\sum_{i=1}^{100}e^{y(x_i-\bar{x})} \approx
e^{y\bar{x}}\sum_{i=1}^{100}\left( 1 + y(x_i-\bar{x}) + \frac{1}{2}y^2(x_i-\bar{x})^2 + \cdots + \frac{1}{n!}y^n(x_i-\bar{x})^n\right)
$$

. . .

Interchanging the sums we obtain
$$
e^{y\bar{x}}\left(100 + y\sum_{i=1}^{100}(x_i-\bar{x}) + \cdots + \frac{y^n}{n!}\sum_{i=1}^{100}(x_i - \bar{x})^n)\right) = e^{y\bar{x}}(a_0 + a_1y + \cdots + a_ny^n) = e^{y\bar{x}}p(y)
$$

. . .

With $a_k = \frac{1}{k!}\sum_{i=1}^{100} (x_i - \bar{x})^k$, thus
$$
\log(f_0(y)) \approx e^{y\bar{x}}p(y)
$$

## Improving the Taylor expansion{.smaller}
The error when approximating with an order $n$ polynomial is $o((y(x_i-\bar{x}))^n)$.

. . .

A possible improvement to accuracy could be achieved by introducing
$$
I_{k} = \{i \in \mathbb{N} \mid x_i \in [k-1,k)\} \quad , \quad \bar{x}_k = \frac{1}{|I_k|}\sum_{i \in I_k}x_i
$$

. . .

And utilizing that
$$
\sum_{i=1}^{100} e^{yx_i} = \sum_{k = 1}^{K}\sum_{i \in I_k} e^{yx_i} \approx \sum_{k = 1}^K e^{y\bar{x}_k}p_k(y)
$$
Where
$$
p_k(y) = \sum_{h=0}^{n_k} a_{k,h} y^h \quad , \quad a_{k,h} = \frac{1}{h!}\sum_{i \in I_k}(x_i - \bar{x}_k)^h
$$

## Taylor expansion implementation{.smaller}

:::: {.columns}

::: {.column width="70%"}

```r
polycoef <- function(n, x = poisson$x){
  x_centralized <- x - mean(x)
  0:n %>%
    purrr::map_dbl(.f=function(n){
      sum(x_centralized^n)
    }) / factorial(0:n)
}
```

```r
vandermonde <- function(y, n, mode = "r"){
  switch(mode,
         "c" = vmC(y,n),
         "c2" = vmC2(y,n),
         "r" = outer(y, 0:n, `^`) %>% t()
         )
}
```

```r{code-line-numbers="15-20"}
poisson_prior_approximation <- function(
    x = poisson$x, z = poisson$z, breaks = c(0,1,2,3,4), K = 4,
    vm_method = "r"){
  xz <- sum(x*z)
  groups <- cut(x, breaks, labels = F)
  x_means <- 1:max(groups) %>%
    purrr::map_dbl(.f = function(k){
      mean(x[groups == k])
    })
  coef <- list(k = 1:max(groups), n = K) %>%
    purrr::pmap(.f = function(k, n){
      polycoef(n, x[groups == k])
    }) %>%
    purrr::reduce(.f = rbind)
  approximation <- function(y){
    VMy <- vandermonde(y, K, vm_method)
    p <- coef %*% VMy
    expmean <- exp(outer(x_means, y))
    y*xz - colSums(p * expmean)
  }
  approximation}
```
:::

::: {.column width="30%"}

$$
\texttt{polycoef} \quad , \quad a_{k,h} = \frac{1}{h!}\sum_{i \in I_k}(x_i - \bar{x}_k)^h
$$

$$
\texttt{coef %*% Vmy} \quad , \quad p_k(y) = \sum_{h=0}^{n_k} a_{k,h} y^h
$$

Collecting terms

$$
\log(f_0(y)) \approx y \sum_{i=1}^{100} x_iz_i - \sum_{k=1}^K e^{y\bar{x}_k}p_k(y)
$$
:::

::::

## Benchmarking the Taylor expansion

```{r Approximate Poisson Prior Benchmark}
#| cache: true
bm <- benchmark_poisson_prior_densities()
bm %>%
  ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
```

## Are we simulating from the correct distribution?{.smaller}

```{r Check that the Taylor expansion is a sufficient approximation - setup}
#| cache: true
#| echo: true

set.seed(0)

ppa_precise <- poisson_prior_approximation(breaks = c(0, 1, 2, 3, 4), K = 4)
ppa_imprecise <- poisson_prior_approximation(breaks = c(0, 4), K = 4)

sampler_ppa_precise <- RandomVariable(log_f = ppa_precise, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()

sampler_ppa_imprecise <- RandomVariable(log_f = ppa_imprecise, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()

exact_sampler <- RandomVariable(log_f = poisson_prior_log_f, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()

for (i in 1:25){
  sampler_ppa_precise(5000, train = T, adapt_enve = T)
  sampler_ppa_imprecise(5000, train = T, adapt_enve = T)
  exact_sampler(5000, train = T, adapt_enve = T)
}
```

## Are we simulating from the correct distribution?{.smaller}

```{r Check that the Taylor expansion is a sufficient approximation - simulation}
#| cache: true
#| echo: true
N <- 1e6
precise_sim <- sampler_ppa_precise(N)
imprecise_sim <- sampler_ppa_imprecise(N)
exact_sim <- exact_sampler(N)
exact_sim_2 <- exact_sampler(N)

precise_dens <- density(precise_sim)
imprecise_dens <- density(imprecise_sim)
exact_dens <- density(exact_sim)

set.seed(NULL)
```

## Are we simulating from the correct distribution?{.smaller}

```{r KS Tests}
#| cache: true
#| echo: true
ks.test(imprecise_sim, exact_sim) # Hypothesis is rejected - Distribution is different
ks.test(precise_sim, exact_sim) # Hypothesis is not rejected - Distributions may be the same
ks.test(exact_sim_2, exact_sim) # Hypothesis is true, but almost rejected!
```

## Are we simulating from the correct distribution?{.smaller}

```{r Plotting results}

make_densdata <- function(dens_obj, label){
  tibble::tibble(
    x = dens_obj$x,
    y = dens_obj$y,
    Sampler = label
  )
}

data <- list(dens_obj = list(precise_dens, imprecise_dens, exact_dens),
             label = list("Grouping, n = 4", "No Grouping, n = 4", "Exact")) %>% 
  purrr::pmap_dfr(.f = make_densdata)

ggplot2::ggplot(data) +
  ggplot2::geom_line(ggplot2::aes(x = x, y=y, color = Sampler)) +
  ggplot2::labs(x = "x", y = "Density")
```

## Profiling the final implementation

```{r Profiling the final implementation, eval = F}
#| cache: true
#| echo: true
set.seed(0)
ppa_loworder <- poisson_prior_approximation(breaks = c(0, 1, 2, 3, 4), K = 4)
loworder_sampler <- RandomVariable(log_f = ppa_loworder, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()
suppressWarnings(
  gauss_sampler <- RandomVariable(log_f = ppa_loworder, log_f_prime = poisson_prior_log_f_prime, support = c(0,1))
  %>%
    GaussianEnvelope() %>% 
    rejection_sampler()
)


for (i in 1:25){
  loworder_sampler(5000, train = T, adapt_enve = T)
}

profvis::profvis(loworder_sampler(1e7, train = F), simplify = F)

profvis::profvis(gauss_sampler(1e7, train = F), simplify = F)

set.seed(NULL)

```
## Profiling the approximate implementation

![Profvis for adaptive envelope](2_poorly_rendered_profile_adapt_optim.png)

![Profvis for gaussian envelope](2_poorly_rendered_profile_gauss_optim.png)

- Exponentiation is still expensive, but a lot of time is also spent computing the Vandermonde matrix

## Implementing the Vandermonde matrix in C++{.smaller}

:::: {.columns}

::: {.column width="50%"}
```cpp
NumericMatrix vmC(NumericVector y, int n){
  NumericMatrix VMy(n + 1, y.size());

  for (int r = 0; r <= n; ++r){
    for (int c = 0; c < y.size(); ++c){
      VMy(r,c) = pow(y[c], r);
    }
  }

  return(VMy);

}
```
:::

::: {.column width="50%"}
```cpp
NumericMatrix vmC2(NumericVector y, int n){
  NumericMatrix VMy(n + 1, y.size());

  for (int c = 0; c < y.size(); ++c){
    VMy(0,c) = 1;
  }

  for (int c = 0; c < y.size(); ++c){
    for (int r = 1; r <= n; ++r){
      VMy(r,c) = VMy(r-1,c) * y[c];
    }
  }

  return(VMy);

}
```
:::

::::

## Benchmarking the Vandermonde implementations

```{r Benchmarking Vandermonde Implementations}
#| cache: false
#| echo: true
set.seed(0)
y <- runif(10000)
set.seed(NULL)
microbenchmark::microbenchmark(
  vandermonde(y, 8, "r"),
  vandermonde(y, 8, "c"),
  vandermonde(y, 8, "c2"),
  check = "equal"
) %>% 
  ggplot2::autoplot()
```

## Benchmarking the rejection sampler

```{r Benchmarking rejection sampler after rcpp vandermonde}
#| cache: true
set.seed(0)
ppa_loworder_vmC2 <- poisson_prior_approximation(breaks = c(0, 1, 2, 3, 4), K = 4, vm_method = "c2")
suppressWarnings(
  gauss_sampler_vmC2 <- RandomVariable(log_f = ppa_loworder_vmC2, log_f_prime = poisson_prior_log_f_prime, support = c(0,1))
  %>%
    GaussianEnvelope() %>% 
    rejection_sampler()
)

ppa_loworder <- poisson_prior_approximation(breaks = c(0, 1, 2, 3, 4), K = 4, vm_method = "r")
suppressWarnings(
  gauss_sampler <- RandomVariable(log_f = ppa_loworder, log_f_prime = poisson_prior_log_f_prime, support = c(0,1))
  %>%
    GaussianEnvelope() %>% 
    rejection_sampler()
)

mb <- microbenchmark::microbenchmark(
  gauss_sampler(1e5),
  gauss_sampler_vmC2(1e5)
)

print(mb)
ggplot2::autoplot(mb)

set.seed(NULL)
```









