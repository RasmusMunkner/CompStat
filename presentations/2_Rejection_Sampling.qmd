---
title: "Rejection Sampling"
author: "Rasmus V. Munkner"
format: revealjs
---

## Problem statement

- Want to sample i.i.d. observations $(X_n)_{n \in \mathbb{N}} \sim f_0$.

- We only know $f_0$ up to a normalizing constant.

. . .

Steps:

- Find an envelope $g$ such that $\alpha f_0 \leq g$.

. . .

- Sample observations $(Y_n)_{n \in \mathbb{N}} \sim g$.

. . .

- Accept/reject observations based on $U_n \leq \alpha \frac{g(Y_n)}{f_0(Y_n)} := \frac{\tilde{g}(Y_n)}{\tilde{f_0}(Y_n)}$

## RandomVariable{.scrollable}
- Distributional information is encoded in *RandomVariable*'s

```{r Demonstrating RandomVariable}
#| echo: true
library(CompStat)
X <- named_rv("normal")
print(X)
plot(X)
Y <- named_rv("poisson prior")
print(Y)
plot(Y)
```

## Envelope{.scrollable}
- Envelopes are defined on top of RandomVariable's

```{r Demonstrating Envelope}
#| echo: true
epa_rv <- named_rv("epanechnikov")

lap_enve <- LaplaceEnvelope(epa_rv)
print(lap_enve)
plot(lap_enve)

gauss_enve <- GaussianEnvelope(epa_rv)
print(gauss_enve)
plot(gauss_enve)

loglinear_enve <-
  LogLinearEnvelope(epa_rv,
                    tangent_points = c(-0.7, -0.1, 0.2, 0.4, 0.8)
                    )
print(loglinear_enve)
plot(loglinear_enve)


```
## Sampler{.scrollable}

```{r Demonstrating rejection_sampler}
#| echo: true
rv_epa <- named_rv("epa")
enve <- LogLinearEnvelope(rv_epa, autoselection_msg = F)
sampler <- rejection_sampler(enve)
set.seed(0)
sim <- sampler(100000)
set.seed(NULL)
```

```{r Plotting rejection sampler results}
x <- seq(-3,3,0.01)
y <- ifelse(abs(x)<1, 3/4 * (1-x^2), 0)
both <- ggplot2::ggplot() +
  ggplot2::geom_histogram(ggplot2::aes(x = sim, ggplot2::after_stat(density)), bins = 75) +
  ggplot2::geom_line(ggplot2::aes(x = x, y = y), color = "red") +
  ggplot2::labs(x = "x", y = "Density") +
  ggplot2::ggtitle("Both")
true_dens <- ggplot2::ggplot() +
  ggplot2::geom_line(ggplot2::aes(x = x, y = y), color = "red") +
  ggplot2::labs(x = "x", y = "Density") +
  ggplot2::ggtitle("True density")
sim_hist <- ggplot2::ggplot() +
  ggplot2::geom_histogram(ggplot2::aes(x = sim, ggplot2::after_stat(density)), bins = 75) +
  ggplot2::labs(x = "x", y = "Density") +
  ggplot2::ggtitle("Simulated data")
gridExtra::grid.arrange(grobs = list(sim_hist, true_dens, both), nrow = 2, ncol = 2)
set.seed(NULL)

```

## Initial Benchmark

```{r Initial Benchmark for envelopes (epanechnikov)}
#| cache: true
bm <- benchmark_envelopes(rv = named_rv("epanechnikov"))
bm %>%
  ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
```

## Benchmarking with the poisson prior

```{r Initial Benchmark for envelopes (poisson prior)}
#| cache: true
suppressWarnings(
  bm <- benchmark_envelopes(N_seq = 4:8, rv = named_rv("poisson prior"))
)
bm %>%
  ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
```

## Profiling the fast implementations{.scrollable}

- Provis does not catch it, but the slow operation is exponentiation.

```{r Profiling Samplers}
#| cache: true
#| echo: true

set.seed(0)

suppressWarnings(
  gauss_sampler <- named_rv("poisson") %>% GaussianEnvelope() %>% rejection_sampler()
)
adapted_sampler <- named_rv("poisson") %>%
  LogLinearEnvelope(autoselection_msg = F) %>%
  rejection_sampler(evalmode = 1)

for (i in 1:50){
  adapted_sampler(10000, train = T, adapt_enve = T)
}

profvis::profvis(
  adapted_sampler(300000),
  simplify = F
)

profvis::profvis(
  gauss_sampler(300000),
  simplify = F
)

set.seed(NULL)

```

## Revisiting the poisson prior{.smaller}
The target log-density is
$$
\log(f_0(y)) = \sum_{i=1}^n yz_ix_i - e^{yx_i} = y\left(\sum_{i=1}^{100} z_ix_i\right) - \sum_{i=1}^{100} e^{yx_i} 
$$

. . .

- Due to the second term, exponentiation has to be performed 100 times.

- We suggest an approximation based on a Taylor-expansion.

## Taylor expansion{.smaller}
We have
$$
\sum_{i=1}^{100}e^{yx_i} =
e^{\bar{x}y}\sum_{i=1}^{100}e^{y(x_i-\bar{x})} \approx
e^{y\bar{x}}\sum_{i=1}^{100}\left( 1 + y(x_i-\bar{x}) + \frac{1}{2}y^2(x_i-\bar{x})^2 + \cdots + \frac{1}{n!}y^n(x_i-\bar{x})^n\right)
$$

. . .

Interchanging the sums we obtain
$$
e^{y\bar{x}}\left(100 + y\sum_{i=1}^{100}(x_i-\bar{x}) + \cdots + \frac{y^n}{n!}\sum_{i=1}^{100}(x_i - \bar{x})^n)\right) = e^{y\bar{x}}(a_0 + a_1y + \cdots + a_ny^n) = e^{y\bar{x}}p(y)
$$

. . .

With $a_k = \frac{1}{k!}\sum_{i=1}^{100} (x_i - \bar{x})^k$, thus
$$
\log(f_0(y)) \approx e^{y\bar{x}}p_n(y)
$$

## Improving the Taylor expansion{.smaller}
The error when approximating with an order $n$ polynomial is $o((y(x_i-\bar{x}))^n)$.

. . .

A possible improvement to accuracy could be achieved by introducing
$$
I_{k} = \{i \in \mathbb{N} \mid x_i \in [k-1,k)\} \quad , \quad \bar{x}_k = \frac{1}{|I_k|}\sum_{i \in I_k}x_i
$$

. . .

And utilizing that
$$
\sum_{i=1}^{100} e^{yx_i} = \sum_{k = 1}^{K}\sum_{i \in I_k} e^{yx_i} \approx \sum_{k = 1}^K e^{y\bar{x}_k}p_k(y)
$$
Where
$$
p_k(y) = \sum_{h=0}^{n_k} a_{k,h} y^h \quad , \quad a_{k,h} = \frac{1}{h!}\sum_{i \in I_k}(x_i - \bar{x}_k)^h
$$

## Taylor expansion implementation{.smaller}

```r{code-line-numbers="1-3|5-12|13-33"}
vandermonde <- function(y, n){
  outer(y, 0:n, `^`) %>% t()
}

polycoef <- function(n, x = poisson$x){
  x_centralized <- x - mean(x)
  0:n %>%
    purrr::map_dbl(.f=function(n){
      sum(x_centralized^n)
    }) / factorial(0:n)
}

poisson_prior_approximation <- function(x = poisson$x, z = poisson$z, breaks = c(0,1,2,3,4), orders = 4){
  xz <- sum(x*z)
  groups <- cut(x, breaks, labels = F)
  x_means <- 1:max(groups) %>%
    purrr::map_dbl(.f = function(k){
      mean(x[groups == k])
    })
  coef <- list(k = 1:max(groups), n = orders) %>%
    purrr::pmap(.f = function(k, n){
      polycoef(n, x[groups == k])
    }) %>%
    purrr::reduce(.f = rbind)
  approximation <- function(y){
    VMy <- vandermonde(y, max(orders))
    p <- coef %*% VMy
    expmean <- exp(outer(x_means, y))
    y*xz - colSums(p * expmean)
  }
  approximation
}
```

## Benchmarking the Taylor expansion

```{r Approximate Poisson Prior Benchmark}
#| cache: true
bm <- benchmark_poisson_prior_densities()
bm %>%
  ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
```

## Are we simulating from the correct distribution?{.scrollable .smaller}

```{r Check that the Taylor expansion is a sufficient approximation}
#| cache: true
#| echo: true

set.seed(0)

ppa_loworder <- poisson_prior_approximation(breaks = c(0, 1, 2, 3, 4), order = 4)
ppa_inaccurate <- poisson_prior_approximation(breaks = c(0, 4), order = 4)

loworder_sampler <- RandomVariable(log_f = ppa_loworder, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()

inaccurate_sampler <- RandomVariable(log_f = ppa_inaccurate, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()

exact_sampler <- RandomVariable(log_f = poisson_prior_log_f, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()

for (i in 1:25){
  loworder_sampler(5000, train = T, adapt_enve = T)
  inaccurate_sampler(5000, train = T, adapt_enve = T)
  exact_sampler(5000, train = T, adapt_enve = T)
}

N <- 1e6
loworder_sim <- loworder_sampler(N)
inaccurate_sim <- inaccurate_sampler(N)
exact_sim <- exact_sampler(N)
exact_sim_2 <- exact_sampler(N)

loworder_dens <- density(loworder_sim)
inaccurate_dens <- density(inaccurate_sim)
exact_dens <- density(exact_sim)

ks.test(inaccurate_sim, exact_sim)
ks.test(loworder_sim, exact_sim)
ks.test(exact_sim_2, exact_sim)

set.seed(NULL)

```

## Visual checks

```{r Plotting results}

make_densdata <- function(dens_obj, label){
  tibble::tibble(
    x = dens_obj$x,
    y = dens_obj$y,
    Sampler = label
  )
}

data <- list(dens_obj = list(loworder_dens, inaccurate_dens, exact_dens),
             label = list("Grouping, n = 4", "No Grouping, n = 4", "Exact")) %>% 
  purrr::pmap_dfr(.f = make_densdata)

ggplot2::ggplot(data) +
  ggplot2::geom_line(ggplot2::aes(x = x, y=y, color = Sampler)) +
  ggplot2::labs(x = "x", y = "Density")
```

## Profiling the final implementation

```{r Profiling the final implementation}
#| cache: true

set.seed(0)
ppa_loworder <- poisson_prior_approximation(breaks = c(0, 1, 2, 3, 4), order = 4)
loworder_sampler <- RandomVariable(log_f = ppa_loworder, log_f_prime = poisson_prior_log_f_prime, support = c(0,1)) %>% 
  LogLinearEnvelope() %>% 
  rejection_sampler()

for (i in 1:25){
  loworder_sampler(5000, train = T, adapt_enve = T)
}

profvis::profvis(loworder_sampler(1e6, train = F), simplify = F)
set.seed(NULL)

```







