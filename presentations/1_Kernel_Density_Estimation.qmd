---
title: "Kernel Density Estimation"
author: "Rasmus V. Munkner"
format: revealjs
code-overflow: wrap
---

## Problem Statement{.smaller}

- Observations $X_1, \ldots , X_n \sim f_0$. $K: \mathbb{R} \rightarrow \mathbb{R}$ a kernel.
- Estimator $$\hat{f}(x) = \frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-x_i}{\hat{h}}\right)$$

. . . 

Challenges

- Compute $\hat{f}$.
- Estimate $\hat{h}$.

## Infrastructure{.smaller}
The fundamental object is a 'CompStatKernel':

```{r CompStatKernel-Print, echo=TRUE}
#| output-location: column
#| echo: TRUE
library(CompStat)
gkern <- CompStatKernel("g")
print(gkern)
```

```{r CompStatKernel-Plot}
#| echo: TRUE
plot(gkern)
```



## Computing $\hat{f}$ in R
```r
eval_kdensR <- function(k, grid, x, bw){
  grid %>%
    purrr::map_dbl(.f=function(z){
      mean(k((z-x)/bw))/bw
    })
}
```

```r
epa_kernel <- function(x){
  res <- numeric(length(x))
  ind <- abs(x) <= 1
  res[!ind] <- 0
  res[ind] <- 3/4 * (1-x[ind]^2)
  res
}

gauss_kernel <- function(x) dnorm(x)
```

## Computing $\hat{f}$ in C++
```cpp{code-line-numbers=""}
NumericVector eval_kdensC(
  String kcode,
  NumericVector grid,
  NumericVector x,
  double bw
  ){

  int m = grid.size();
  int n = x.size();

  NumericVector out(m);

  if (kcode == "Gaussian"){

    for(int i = 0; i < m; ++i){
      for(int j = 0; j < n; ++j){
        out[i] += exp(-pow((grid[i] - x[j])/bw, 2)/2);
      }
      out[i] /= bw * n * sqrt(2*M_PI);
    }

  } else { //Assuming Epanechnikov Kernel
    double tmp;

    for(int i = 0; i < m; ++i){
      for(int j = 0; j < n; ++j){
        tmp = pow((grid[i] - x[j])/bw,2);
        if (tmp < 1){
          out[i] += 1 - tmp;
        }
      }
      out[i] /= bw * n * 4 / 3;
    }

  }

  return out;

}
```

## Testing implementation of $\hat{f}$

- We are using *testthat* for unit tests

```{r Running Tests}
#| echo: true
devtools::test(filter="kernels")
```

## Testing implementation of $\hat{f}$

```r{code-line-numbers="2-20|22-29|32-46|48-61"}
# CompStatKernel
test_that("Kernel Initializes correctly", {
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  expect_equal(class(g), "CompStatKernel")
  expect_equal(class(e), "CompStatKernel")

  g <- CompStatKernel(g)
  e <- CompStatKernel(e)
  expect_equal(class(g), "CompStatKernel")
  expect_equal(class(e), "CompStatKernel")

  g <- CompStatKernel("GGGGGGGaussian")
  e <- CompStatKernel("e______ppaaaanne   ")
  expect_equal(class(g), "CompStatKernel")
  expect_equal(class(e), "CompStatKernel")

  expect_error(CompStatKernel(NULL))
  expect_error(CompStatKernel(1))
})

test_that("Implemented kernels are calculated correctly", {
  grid <- seq(-2, 2, 0.001)
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")

  expect_equal(g$kernel(grid), dnorm(grid))
  expect_equal(e$kernel(grid), ifelse(abs(grid) < 1, 3/4*(1-grid^2), 0))
})

# kernel_density
test_that("Kernel density estimation is consistent with stats::density.
          Be aware that stats::density rescales its kernels.", {
  set.seed(0)
  x <- rnorm(1000)
  bw <- 0.1
  tol <- 0.01
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  ref_g <- density(x, bw = bw, kernel = "g")
  ref_e <- density(x, bw = bw, kernel = "e")
  test_g <- kernel_density(g, x, bw, grid = ref_g$x)
  test_e <- kernel_density(e, x, bw / sqrt(e$sigma2), grid = ref_e$x)
  expect_true(all(abs(ref_g$y - test_g) < tol))
  expect_true(all(abs(ref_e$y - test_e) < tol)) # Breaks around (tol < 0.00142)
})

test_that("Kernel density estimates are consistent
          across R and C++ implementations", {
  set.seed(0)
  x <- rnorm(1000)
  bw <- 0.1
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  gR <- kernel_density(g, x, bw, method = "r")
  eR <- kernel_density(e, x, bw, method = "r")
  gC <- kernel_density(g, x, bw, method = "c")
  eC <- kernel_density(e, x, bw, method = "c")
  expect_equal(gR, gC)
  expect_equal(eR, eC)
})

# l2norm.CompStatKernel

test_that("L2-Norm estimation works (baseline are matrix implementations)", {
  set.seed(0)
  x <- rnorm(100)
  set.seed(NULL)
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  expect_equal(l2norm(g, x, 1), l2norm(g, x, 1, method = "matrix"))
  expect_equal(l2norm(e, x, 1), l2norm(e, x, 1, method = "matrix"))
})



#ACommentToEnsureWeCanSeeAllTheCode
#MoreComments
```

## Benchmarking $\hat{f}$

```{r Benchmarking fhat}
#| echo: false
#| cache: true
bm <- benchmark_kernel_density()
bm %>% 
ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) + 
ggplot2::geom_line() +
ggplot2::geom_point() +
ggplot2::facet_wrap(~Kernel)
```
## Estimating $\hat{h}${.smaller}

:::{#thm-AMISE}

## AMISE
For a $C^2$-density $f_0$, the oracle bandwidth is
$$
h = \left(\frac{||K||_2^2}{||f_0''||_2^2 \sigma_K^4}\right)^{\frac{1}{5}}n^{-\frac{1}{5}}
$$

:::

:::{#lem-L2-Norm}
Let $f = \frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-x_i}{h}\right)$. Then
$$
||f''||_2^2 = \frac{1}{n^2h^6}\sum_{i=1}^n \sum_{j=1}^n K''\left(\frac{x-x_i}{h}\right)K''\left(\frac{x-x_j}{h}\right)
$$
:::

:::{#def-CV_Error}
We say the CV error for a density estimator $\hat{f}_h$ is
$$
\ell_{CV}(h) = \sum_{i=1}^n \log(\hat{f}_h^{-i}) \quad \text{with} \quad \hat{f}_h^{-i} = \frac{1}{h|I^{-i}|}\sum_{j \in I^{-i}}K\left(\frac{x-x_j}{h}\right)
$$
:::
## Estimating $\hat{h}$

```r{code-line-numbers="1-11|12-16|18-27|29-37|43-56|58-64|66-75|77-88"}
iter_bw_est <- function(x,
                        maxiter = 3L,
                        kernel = "e",
                        bw0 = NULL,
                        cv_k = 3,
                        evaluation_method = "c",
                        l2_method = "default",
                        tol = 1e-4,
                        reltol = 1e-2,
                        cvtol = -Inf
                        ){
  # Initialize containers ------------------------------------------------------
  kernel <- CompStatKernel(kernel)
  bw_seq <- rep(NA, maxiter + 1)
  fnorm_seq <- rep(NA, maxiter+1)
  cv_seq <- rep(NA, maxiter + 1)

  # Initial choice of bandwidth-------------------------------------------------
  if (is.null(bw0)){
    if (sd(x) == 0){
      stop("Standard deviation of x is 0.
           Kernel density estimation is not meaningful.")
    }
    bw_seq[1] <- 0.9 * sd(x) * length(x)^(1/5) # Silverman
  } else {
    bw_seq[1] <- bw0 # Pre-specified guess
  }

  # Check if cross-validation should be performed-------------------------------
  cv_enabled <- FALSE
  if (cv_k == as.integer(cv_k) & cv_k > 1){
    cv_enabled <- TRUE
    cv_seq[1] <- cvscore(kernel, x, bw_seq[1], cv_k, method=evaluation_method)
  } else if (cv_k != as.integer(cv_k)){
    message("Number of CV splis (cv_k) is not an integer.
            CV will be skipped. Set cv_k = 0 to supress this message.")
  }

  # Pre-computed quantities-----------------------------------------------------
  sigmaK4 <- kernel$sigma2^2
  n <- length(x)
  exit_code <- "Maximum number iterations reached"

  # Iterate --------------------------------------------------------------------
  for (i in 1:maxiter){

    # Update bandwidth
    fnorm_seq[i] <- l2norm(kernel, x, bw_seq[i], method = l2_method)
    bw_seq[i+1] <- (kernel$l2norm / fnorm_seq[i] / sigmaK4 / n)^(1/5)

    # Cross-validation
    if (cv_k > 0){
      cv_seq[i+1] <- cvscore(
        kernel, x, bw_seq[i+1], cv_k,
        method=evaluation_method
        )
    }

    # Bandwidth stopping criteria
    if (
      abs(bw_seq[i+1] - bw_seq[i]) < tol ||
      abs(bw_seq[i+1]-bw_seq[i]) / bw_seq[i] < reltol
        ){
      exit_code <- "Bandwidth tol reached"
      break
    }

    # CV stopping criteria
    if (cv_enabled){
      if (
        !is.finite(cv_seq[i+1]) ||
        cv_seq[i+1] - cv_seq[i] < cvtol
        ){
        if (!is.finite(cv_seq[i+1])){
          exit_code <- "Infinite CV score"
        } else {
          exit_code <- "CV tol reached"
        }
        break
      }
    }
  }

  # Return results -------------------------------------------------------------
  estimates <- tibble::tibble(
    bw=bw_seq[!is.na(bw_seq)],
    fnorm=fnorm_seq[!is.na(bw_seq)]
  )
  if (cv_k > 0){
    estimates <- estimates %>%
      dplyr::mutate(cvscore=cv_seq[!is.na(bw_seq)])
  }

  structure(
    list(
      estimates = estimates,
      x = x,
      kernel = kernel,
      exit_code = exit_code
    ),
    class = "IterBwEstimate"
  )

}
```

## Quality of $\hat{f}$

```{r Checking Density Estimates}
#| cache: true
#| echo: true
set.seed(0)
n <- 3000
z <- rbinom(n, size = 1, prob = 0.3)
x <- z*rnorm(n) + (1-z) * rnorm(n, mean = 4)
set.seed(NULL)
res_e <- iter_bw_est(x, maxiter = 30, kernel = "e", cv_k = 5)
res_g <- iter_bw_est(x, maxiter = 30, kernel = "g", cv_k = 5)
```

## Quality of $\hat{f}$

```{r Displaying test 1}
#| echo: true
plot(res_e)
```

## Quality of $\hat{f}$

```{r Displaying test 2}
#| echo: true
plot(res_g)
```

## Profiling $\hat{h}$
```{r Profiling iter_bw_est CV}
#| cache: true
#| echo: false
set.seed(0)
u <- runif(3000)
profvis::profvis(iter_bw_est(u, maxiter = 5, kernel="e", l2_method = "matrix", cv_k = 3), simplify=F)
set.seed(NULL)
```

## Conclusions from profiling $\hat{h}$
- Both calculation of the $L_2$-norm and CV-score are costly
- CV-score we already optimized as much as we could.
- CV-score is not essential to the method.
- Thus it will be disabled if we set $\texttt{cv_k} = 0$

## Calculating $||f||_2^2$

:::{#lemma-Epanechnikov}
For the Epanechnikov kernel with bandwidth $h$, it holds that
$$
||\hat{f}''||_2^2 = \frac{9}{4}\frac{1}{n^2h^6} \sum_{i=1}^{n}\sum_{j=1}^n (2h-|x_i-x_j|)^+
$$
:::

- Many terms in the sum are 0.
- Working with matrix $\delta_{ij} = (x_i-x_j)_{i,j \in \{1, \ldots , n\}}$ requires $O(n^2)$ memory.

## Naive Implementation of $||f||_2^2$

```r
epanechnikov_l2norm_matrix <- function(x, r){
  x <- 9/4 * x / r^6 / length(x)^2
  r <- 9/4 * r / r^6 / length(x)^2
  rep_x <- matrix(rep(x, length(x)), nrow=length(x))
  diff_matrix <- 2*r - abs(rep_x - t(rep_x))
  sum(diff_matrix[diff_matrix > 0])
}
```

## C++ Implementation of $||f||_2^2${.smaller}

```cpp{code-line-numbers="1|2|6-7|8-20|21-25"}
double epanechnikov_l2norm_runningC(NumericVector x, double r){
  std::sort(x.begin(), x.end());
  int n = x.size();
  x.push_back(R_PosInf);
  double results = 0;
  int i = 1;
  int j = 0;
  while(j < n-1){
    if(i <= j || x[i+1] - x[j] < 2*r){
      ++i;
    } else if (x[i] - x[j] < 2*r) {
      for(int k = j+1; k <= i; ++k){
        results -= x[k];
      }
      results = results + (2*r + x[j]) * (i - j);
      ++j;
    } else {
      ++j;
    }
  }
  results = 2*results + 2*r*n;
  return(
    results * 9 / 4 / pow(n,2) / pow(r, 6)
  );
}
```

## Checking that results are correct

```{r Running Tests for l2norm}
#| echo: true
devtools::test(filter="epanechnikov_l2norm")
```

## Checking that results are correct

```r{code-line-numbers="1-16|88-94"}
# Matrix-method
test_that("Matrix method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_matrix(0, 1),
    4.5
  )
})

test_that("Matrix method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 1),
    4.5 / l
  )
})

# Can't test the matrix method for 'real' data, since we have nothing to compare
# it to. It serves as our most trusted (and slowest) baseline

# Running
test_that("Running method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_running(0, 1),
    4.5
  )
})

test_that("Running method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_running(x, 1),
    4.5 / l
  )
})

test_that("Running method (real data)", {
  x <- rnorm(1000)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 0.1),
    epanechnikov_l2norm_running(x, 0.1)
  )
})

# Binning
test_that("Binning method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_binning(0, 1),
    4.5
  )
})

test_that("Binning method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_binning(x, 1),
    4.5 / l
  )
})

test_that("Binning method (real data)", {
  x <- rnorm(1000)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 0.1),
    epanechnikov_l2norm_binning(x, 0.1)
  )
})

# Cpp running
test_that("Running cpp method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_runningC(0, 1),
    4.5
  )
})

test_that("Running cpp method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_runningC(x, 1),
    4.5 / l
  )
})

test_that("Running cpp method (real data)", {
  x <- rnorm(1000)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 0.1),
    epanechnikov_l2norm_runningC(x, 0.1)
  )
})
```

## Benchmarking $||f||_2^2$

```{r Benchmarking L2Norm}
#| cache: true
set.seed(0)
bm <- benchmark_epanechnikov_l2norm()
bm %>%
ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
set.seed(NULL)
```

## Source code for kernels{.scrollable}

```r
#' Instantiate a CompStatKernel
#'
#' The valid codes are gaussian/normal, rectangular/uniform, epanechnikov and triangular.
#' These can also be accessed by the corresponding 1-letter codes.
#'
#' @param kernel_code An alias for the desired kernel.
#'
#' @return The kernel corresponding to the given input.
#' @export
#'
#' @examples
#' set.seed(0)
#' x <- rnorm(100000)
#' ekern <- CompStatKernel("e")
#' profvis::profvis(kernel_density(ekern, x, 0.1, method = "r"))
CompStatKernel <- function(kernel_code){

  if("CompStatKernel" %in% class(kernel_code)){
    return(kernel_code)
  } else {
    kernel_code <- kernel_code %>% substr(1,1) %>% tolower()
    valid_kernels <- c("n", "g", "e")
    if (!(kernel_code %in% valid_kernels)){
      stop(purrr::reduce(.x = c("Invalid kernel code. Valid codes are:", valid_kernels), .f = paste))
    }
  }

  name <- switch(kernel_code,
    n= ,
    g= "Gaussian",
    u= ,
    r= "Rectangular",
    t= "Triangular",
    e= "Epanechnikov"
  )
  kernel <- switch(kernel_code,
         n= ,
         g= dnorm,
         u= ,
         r= function(z){ifelse(abs(z)<1, 1/2, 0)},
         t= function(z){ifelse(abs(z)<1, 1-abs(z), 0)},
         #e= function(z){ifelse(abs(z)<1, 3/4 * (1-z^2), 0)}, # Old implementation, ~30% slower
         e= epa_kernel
  )
  hessian <- switch(kernel_code,
         n= ,
         g= function(z){(z^2 - 1) * dnorm(z)},
         u= ,
         r= function(z){rep(0, length(z))},
         t= function(z){rep(0, length(z))},
         e= function(z){ifelse(abs(z) < 1, -3/2, 0)}
  )
  l2norm <- switch(kernel_code,
                   n = ,
                   g = 3/8 / sqrt(pi),
                   u = ,
                   r = 1/2,
                   t = 2/3,
                   e = 3/5
                   )
  sigma2 <- switch(kernel_code,
                  n = ,
                  g = 1,
                  u = ,
                  r = 1/3,
                  t = 1/6,
                  e = 1/5
                  )
  return(
    structure(
      list(
        name = name,
        kernel = kernel,
        hessian = hessian,
        l2norm = l2norm,
        sigma2 = sigma2
      ),
      class = "CompStatKernel"
    )
  )
}

#' Plotting function for CompStatKernel's
#'
#' @param kernel A CompStatKernel
#' @param x A numeric vector with the points the kernel is evaluated in
#'
#' @return A ggplot showing the kernel
#' @export
#'
#' @examples
#' plot(CompStatKernel("g"))
#' plot(CompStatKernel("e"))
plot.CompStatKernel <- function(kernel, x = seq(-2,2,0.01)){
  ggplot2::ggplot(NULL, mapping = ggplot2::aes(x = x, y = kernel$kernel(x))) +
    ggplot2::geom_line() +
    ggplot2::labs(x = "x", y = "K(x)")
}

#' Printing method for CompStatKernel's
#'
#' @param kernel
#'
#' @return Prints a tibble of information on the CompStatKernel
#' @export
#'
#' @examples
#' "g" %>% CompStatKernel()
#' "e" %>% CompStatKernel()
print.CompStatKernel <- function(kernel){
  print("CompStatKernel", quote = F)
  print(paste0("Type: ", kernel$name), quote = F)
  print(paste0("L2: ", round(kernel$l2norm, 4)), quote = F)
  print(paste0("Sigma2: ", round(kernel$sigma2, 4)), quote = F)
}



#' Fast implementation of the Epanechnikov kernel
#'
#' It is possible to make a comparable version of this function where x itself is used to store the values.
#' This is slightly faster if the kernel is evaluated many time on small inputs, but otherwise slightly slower
#'
#' @param x Vector for the kernel to be evaluated on
#'
#' @return The values of the kernel for the given input
epa_kernel <- function(x){
  res <- numeric(length(x))
  ind <- abs(x) <= 1
  res[!ind] <- 0
  res[ind] <- 3/4 * (1-x[ind]^2)
  res
}

#' Compiles kernel density estimate
#'
#' @param x Data used for density estimation
#' @param bw Bandwidth
#' @param kernel Kernel used for smoothing. Defaults to Gaussian
#'
#' @return The estimated density function
#' @export
#'
#' @examples
#' set.seed(0)
#' x <- rnorm(1000)
#' set.seed(NULL)
#' grid <- density(x, 0.2, kernel = "g")$x
#'
#' microbenchmark::microbenchmark(
#' kernel_density("e", x, 0.2, method="c", grid = grid),
#' kernel_density("e", x, 0.2, method="r", grid = grid),
#' density(x, 0.2 * sqrt(CompStatKernel("e")$sigma2), kernel="e")
#' )
#'
#' microbenchmark::microbenchmark(
#' kernel_density("g", x, 0.2, method="c"),
#' kernel_density("g", x, 0.2, method="r"),
#' density(x, 0.2, kernel="g")
#' )
#'
#'
kernel_density <- function(kernel, x, bw,
                           grid = NULL, n = 512, from = NULL, to = NULL, cut = 3, method = "c",
                           return_grid = F
                           ){

  kernel <- CompStatKernel(kernel)

  if (is.null(grid)){
    rg <- range(x)
    if (is.null(from)){
      from <- rg[1] - cut*bw
    }
    if (is.null(to)){
      to <- rg[2] + cut*bw
    }
    grid <- seq(from, to, length.out=n)
  }

  if (return_grid){
    return(grid)
  }

  if (tolower(method) == "r"){
    return(eval_kdensR(kernel$kernel, grid, x, bw))
  }
  if (tolower(method) == "c"){
    if (kernel$name %in% c("Gaussian", "Epanechnikov")){
      return(eval_kdensC(kernel$name, grid, x, bw))
    } else {
      stop("Method 'c' is only implemented for Gaussian and Epanechnikov kernels.")
    }
  }

  stop("Must specify 'method' as either 'r' or 'c'.")

}

#' Benchmark the kernel density evaluation implementations
#'
#' @return A data frame containing benchmark information
#' @export
#'
#' @examples
#' bm <- benchmark_kernel_density()
#' bm %>%
#' bm %>%
#'  ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
#'  ggplot2::geom_line() +
#'  ggplot2::geom_point() +
#'  ggplot2::facet_wrap(~Kernel)
benchmark_kernel_density <- function(N_seq = 3:12, bw = 0.2){

  set.seed(0)
  xs <- N_seq %>% purrr::map(.f = function(n){rnorm(2^n)})
  set.seed(NULL)

  bm <- N_seq %>%
    purrr::imap_dfr(.f = function(n, i){
      purrr::map_dfr(.x = c("gaussian", "epanechnikov"), .f = function(kernel){
        calls <- list(
          call("kernel_density", kernel = kernel, x = xs[[i]], bw=bw, method = "c"),
          call("kernel_density", kernel = kernel, x = xs[[i]], bw=bw, method = "r"),
          call("density", kernel = kernel, x = xs[[i]], bw=bw*sqrt(CompStatKernel(kernel)$sigma2))
        )
        names(calls) <- c("cpp", "r", "stats::density")
        microbenchmark::microbenchmark(
          list=calls
        ) %>%
          as.data.frame() %>%
          dplyr::group_by(expr) %>%
          dplyr::summarise(Time = median(time)) %>%
          dplyr::mutate(Method = expr, Kernel = kernel, N = 2^n) %>%
          dplyr::select(-c(expr))
      })
    })
  bm
}

#' Implementation of kernel density evaluation in R.
#'
#' @param k The kernel evaluation function
#' @param grid The grid of values to evaluate the estimated density in
#' @param x The data points used to estimate the density
#' @param bw The bandwidth
#'
#' @return The estimated density evaluated at the grid points
#'
eval_kdensR <- function(k, grid, x, bw){
  grid %>%
    purrr::map_dbl(.f=function(z){
      mean(k((z-x)/bw))/bw
    })
}

l2norm <- function(f, ...){
  UseMethod("l2norm")
}

cvscore <- function(kernel, x, r, ...){
  UseMethod("cvscore")
}

#' Calculate an estimate of the L2-norm for the current kernel approximation
#'
#' @param x The data used for estimation
#' @param kernel_code The name of the kernel used
#' @param r The bandwidth
#' @param method A string. Specifies the method used to calculate the L2-Norm.
#'
#' @return An estimate of the L2-norm for the current kernel approximation
#' @export
#'
#' @examples
#' set.seed(0)
#' x <- rnorm(100)
#' H <- CompStatKernel("g")
#' l2norm(H, x, 1)
#' set.seed(NULL)
l2norm.CompStatKernel <- function(kernel, x, r, method = "default"){
  result <- NULL
  if (kernel$name == "Epanechnikov"){
    result <- switch(tolower(method),
           matrix=epanechnikov_l2norm_matrix(x, r),
           default=,
           c=epanechnikov_l2norm_runningC(x, r),
           running=epanechnikov_l2norm_running(x, r),
           binning=epanechnikov_l2norm_binning(x, r),
           NULL
           )
  } else if (kernel$name == "Gaussian"){
    result <- switch(tolower(method),
                     default=,
                     matrix=gaussian_l2norm_matrix(x, r),
                     integrate=gaussian_l2norm_integrate(x, r),
                     NULL
    )
  }
  if (is.null(result)){
    stop("No implementation for the supplied method/kernel combination is available.")
  }
  return(result)
}

#' Compute cross-validated out-of-sample loglikelihood for kernel smoother
#'
#' @param x A numeric vector of data points
#' @param bw The desired bandwidth
#' @param kernel_code The abbreviation for the kernel to use for smoothing
#' @param k The number of folds for cross validation
#' @param ... Additional arguments passed to kernel_density.
#'
#' @return The negative out-of-sample loglikelihood for the kernel smoother
#' @export
#'
#' @examples
#' x <- rnorm(1000)
#' bw <- seq(0.01, 1, 0.01)
#' g <- CompStatKernel("g")
#' scores <- bw %>% purrr::map_dbl(.f = cvscore, kernel = g, x = x)
#' plot(bw, scores, type = "l")
cvscore.CompStatKernel <- function(kernel, x, bw, k = 5, ...){
  set.seed(0)
  splits <- sample(1:k, length(x), replace = T)
  set.seed(NULL)
  scores <- numeric(k)
  for (i in 1:k){
    scores[i] <- -sum(log(kernel_density(
      kernel,
      x=x[splits != i],
      bw=bw,
      grid=x[splits == i],
      ...
      )))
  }
  mean(scores)
}

```





