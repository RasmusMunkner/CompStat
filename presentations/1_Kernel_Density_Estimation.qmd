---
title: "Kernel Density Estimation"
author: "Rasmus V. Munkner"
format: revealjs
code-overflow: wrap
---

## Problem Statement{.smaller}

- Observations $X_1, \ldots , X_n \sim f_0$. $K: \mathbb{R} \rightarrow \mathbb{R}$ a kernel.
- Estimator $$\hat{f}(x) = \frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-x_i}{\hat{h}}\right)$$

. . . 

Challenges

- Compute $\hat{f}$.
- Estimate $\hat{h}$.

## Infrastructure{.smaller}
The fundamental object is a 'CompStatKernel':

```{r CompStatKernel-Print, echo=TRUE}
#| output-location: column
#| echo: TRUE
library(CompStat)
gkern <- CompStatKernel("g")
print(gkern)
```

```{r CompStatKernel-Plot}
#| echo: TRUE
plot(gkern)
```



## Computing $\hat{f}$ in R
```r
eval_kdensR <- function(k, grid, x, bw){
  grid %>%
    purrr::map_dbl(.f=function(z){
      mean(k((z-x)/bw))/bw
    })
}
```

```r
epa_kernel <- function(x){
  res <- numeric(length(x))
  ind <- abs(x) <= 1
  res[!ind] <- 0
  res[ind] <- 3/4 * (1-x[ind]^2)
  res
}

gauss_kernel <- function(x) dnorm(x)
```

## Computing $\hat{f}$ in C++{.smaller}
```cpp
NumericVector eval_kdensC(
  String kcode,
  NumericVector grid,
  NumericVector x,
  double bw
  ){

  int m = grid.size();
  int n = x.size();

  NumericVector out(m);

  if (kcode == "Gaussian"){

    for(int i = 0; i < m; ++i){
      for(int j = 0; j < n; ++j){
        out[i] += exp(-pow((grid[i] - x[j])/bw, 2)/2);
      }
      out[i] /= bw * n * sqrt(2*M_PI);
    }

  } else { //Assuming Epanechnikov Kernel
    double tmp;

    for(int i = 0; i < m; ++i){
      for(int j = 0; j < n; ++j){
        tmp = pow((grid[i] - x[j])/bw,2);
        if (tmp < 1){
          out[i] += 1 - tmp;
        }
      }
      out[i] /= bw * n * 4 / 3;
    }

  }

  return out;

}
```

## Testing implementation of $\hat{f}$

- We are using *testthat* for unit tests

```{r Running Tests}
#| echo: true
devtools::test(filter="kernels")
```

## Testing implementation of $\hat{f}$

```r{code-line-numbers="2-20|22-29|32-46|48-61"}
# CompStatKernel
test_that("Kernel Initializes correctly", {
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  expect_equal(class(g), "CompStatKernel")
  expect_equal(class(e), "CompStatKernel")

  g <- CompStatKernel(g)
  e <- CompStatKernel(e)
  expect_equal(class(g), "CompStatKernel")
  expect_equal(class(e), "CompStatKernel")

  g <- CompStatKernel("GGGGGGGaussian")
  e <- CompStatKernel("e______ppaaaanne   ")
  expect_equal(class(g), "CompStatKernel")
  expect_equal(class(e), "CompStatKernel")

  expect_error(CompStatKernel(NULL))
  expect_error(CompStatKernel(1))
})

test_that("Implemented kernels are calculated correctly", {
  grid <- seq(-2, 2, 0.001)
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")

  expect_equal(g$kernel(grid), dnorm(grid))
  expect_equal(e$kernel(grid), ifelse(abs(grid) < 1, 3/4*(1-grid^2), 0))
})

# kernel_density
test_that("Kernel density estimation is consistent with stats::density.
          Be aware that stats::density rescales its kernels.", {
  set.seed(0)
  x <- rnorm(1000)
  bw <- 0.1
  tol <- 0.01
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  ref_g <- density(x, bw = bw, kernel = "g")
  ref_e <- density(x, bw = bw, kernel = "e")
  test_g <- kernel_density(g, x, bw, grid = ref_g$x)
  test_e <- kernel_density(e, x, bw / sqrt(e$sigma2), grid = ref_e$x)
  expect_true(all(abs(ref_g$y - test_g) < tol))
  expect_true(all(abs(ref_e$y - test_e) < tol)) # Breaks around (tol < 0.00142)
})

test_that("Kernel density estimates are consistent
          across R and C++ implementations", {
  set.seed(0)
  x <- rnorm(1000)
  bw <- 0.1
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  gR <- kernel_density(g, x, bw, method = "r")
  eR <- kernel_density(e, x, bw, method = "r")
  gC <- kernel_density(g, x, bw, method = "c")
  eC <- kernel_density(e, x, bw, method = "c")
  expect_equal(gR, gC)
  expect_equal(eR, eC)
})

# l2norm.CompStatKernel

test_that("L2-Norm estimation works (baseline are matrix implementations)", {
  set.seed(0)
  x <- rnorm(100)
  set.seed(NULL)
  g <- CompStatKernel("g")
  e <- CompStatKernel("e")
  expect_equal(l2norm(g, x, 1), l2norm(g, x, 1, method = "matrix"))
  expect_equal(l2norm(e, x, 1), l2norm(e, x, 1, method = "matrix"))
})



#ACommentToEnsureWeCanSeeAllTheCode
#MoreComments
```

## Benchmarking $\hat{f}$

```{r Benchmarking fhat}
#| echo: false
#| cache: true
bm <- benchmark_kernel_density()
bm %>% 
ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) + 
ggplot2::geom_line() +
ggplot2::geom_point() +
ggplot2::facet_wrap(~Kernel)
```
## Estimating $\hat{h}${.smaller}

:::{#thm-AMISE}

## AMISE
For a $C^2$-density $f_0$, the oracle bandwidth is
$$
h = \left(\frac{||K||_2^2}{||f_0''||_2^2 \sigma_K^4}\right)^{\frac{1}{5}}n^{-\frac{1}{5}}
$$

:::

:::{#lem-L2-Norm}
Let $f = \frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-x_i}{h}\right)$. Then
$$
||f''||_2^2 = \frac{1}{n^2h^6}\sum_{i=1}^n \sum_{j=1}^n K''\left(\frac{x-x_i}{h}\right)K''\left(\frac{x-x_j}{h}\right)
$$
:::

:::{#def-CV_Error}
We say the CV error for a density estimator $\hat{f}_h$ is
$$
\ell_{CV}(h) = \sum_{i=1}^n \log(\hat{f}_h^{-i}) \quad \text{with} \quad \hat{f}_h^{-i} = \frac{1}{h|I^{-i}|}\sum_{j \in I^{-i}}K\left(\frac{x-x_j}{h}\right)
$$
:::
## Estimating $\hat{h}$

```r{code-line-numbers="1-11|12-16|18-27|29-37|43-56|58-64|66-75|77-88"}
iter_bw_est <- function(x,
                        maxiter = 3L,
                        kernel = "e",
                        bw0 = NULL,
                        cv_k = 3,
                        evaluation_method = "c",
                        l2_method = "default",
                        tol = 1e-4,
                        reltol = 1e-2,
                        cvtol = -Inf
                        ){
  # Initialize containers ------------------------------------------------------
  kernel <- CompStatKernel(kernel)
  bw_seq <- rep(NA, maxiter + 1)
  fnorm_seq <- rep(NA, maxiter+1)
  cv_seq <- rep(NA, maxiter + 1)

  # Initial choice of bandwidth-------------------------------------------------
  if (is.null(bw0)){
    if (sd(x) == 0){
      stop("Standard deviation of x is 0.
           Kernel density estimation is not meaningful.")
    }
    bw_seq[1] <- 0.9 * sd(x) * length(x)^(1/5) # Silverman
  } else {
    bw_seq[1] <- bw0 # Pre-specified guess
  }

  # Check if cross-validation should be performed-------------------------------
  cv_enabled <- FALSE
  if (cv_k == as.integer(cv_k) & cv_k > 1){
    cv_enabled <- TRUE
    cv_seq[1] <- cvscore(kernel, x, bw_seq[1], cv_k, method=evaluation_method)
  } else if (cv_k != as.integer(cv_k)){
    message("Number of CV splis (cv_k) is not an integer.
            CV will be skipped. Set cv_k = 0 to supress this message.")
  }

  # Pre-computed quantities-----------------------------------------------------
  sigmaK4 <- kernel$sigma2^2
  n <- length(x)
  exit_code <- "Maximum number iterations reached"

  # Iterate --------------------------------------------------------------------
  for (i in 1:maxiter){

    # Update bandwidth
    fnorm_seq[i] <- l2norm(kernel, x, bw_seq[i], method = l2_method)
    bw_seq[i+1] <- (kernel$l2norm / fnorm_seq[i] / sigmaK4 / n)^(1/5)

    # Cross-validation
    if (cv_k > 0){
      cv_seq[i+1] <- cvscore(
        kernel, x, bw_seq[i+1], cv_k,
        method=evaluation_method
        )
    }

    # Bandwidth stopping criteria
    if (
      abs(bw_seq[i+1] - bw_seq[i]) < tol ||
      abs(bw_seq[i+1]-bw_seq[i]) / bw_seq[i] < reltol
        ){
      exit_code <- "Bandwidth tol reached"
      break
    }

    # CV stopping criteria
    if (cv_enabled){
      if (
        !is.finite(cv_seq[i+1]) ||
        cv_seq[i+1] - cv_seq[i] < cvtol
        ){
        if (!is.finite(cv_seq[i+1])){
          exit_code <- "Infinite CV score"
        } else {
          exit_code <- "CV tol reached"
        }
        break
      }
    }
  }

  # Return results -------------------------------------------------------------
  estimates <- tibble::tibble(
    bw=bw_seq[!is.na(bw_seq)],
    fnorm=fnorm_seq[!is.na(bw_seq)]
  )
  if (cv_k > 0){
    estimates <- estimates %>%
      dplyr::mutate(cvscore=cv_seq[!is.na(bw_seq)])
  }

  structure(
    list(
      estimates = estimates,
      x = x,
      kernel = kernel,
      exit_code = exit_code
    ),
    class = "IterBwEstimate"
  )

}
```

## Quality of $\hat{f}${.scrollable}

```{r Checking Density Estimates}
#| cache: true
#| echo: true
set.seed(0)
n <- 3000
z <- rbinom(n, size = 1, prob = 0.3)
x <- z*rnorm(n) + (1-z) * rnorm(n, mean = 4)
res_e <- iter_bw_est(x, maxiter = 30, kernel = "e", cv_k = 5)
res_g <- iter_bw_est(x, maxiter = 30, kernel = "g", cv_k = 5)
plot(res_e)
plot(res_g)
set.seed(NULL)
```


## Profiling $\hat{h}$
```{r Profiling iter_bw_est CV}
#| cache: true
#| echo: true
set.seed(0)
u <- runif(3000)
profvis::profvis(iter_bw_est(u, maxiter = 5, kernel="e", l2_method = "matrix", cv_k = 3), simplify=F)
set.seed(NULL)
```

## Conclusions from profiling $\hat{h}$
- Both calculation of the $L_2$-norm and CV-score are costly
- CV-score we already optimized as much as we could.
- CV-score is not essential to the method.

## Calculating $||f||_2^2$

:::{#lemma-Epanechnikov}
The for the Epanechnikov kernel with bandwidth $h$, it holds that
$$
||\hat{f}''||_2^2 = \frac{9}{4}\frac{1}{n^2h^6} \sum_{i=1}^{n}\sum_{j=1}^n (2h-|x_i-x_j|)^+
$$
:::

- Many terms in the sum are 0.
- Working with matrix $\delta_{ij} = (x_i-x_j)_{i,j \in \{1, \ldots , n\}}$ requires $O(n^2)$ memory.

## Naive Implementation of $||f||_2^2$

```r
epanechnikov_l2norm_matrix <- function(x, r){
  x <- 9/4 * x / r^6 / length(x)^2
  r <- 9/4 * r / r^6 / length(x)^2
  rep_x <- matrix(rep(x, length(x)), nrow=length(x))
  diff_matrix <- 2*r - abs(rep_x - t(rep_x))
  sum(diff_matrix[diff_matrix > 0])
}
```

## C++ Implementation of $||f||_2^2${.smaller}

```cpp
double epanechnikov_l2norm_runningC(NumericVector x, double r){
  std::sort(x.begin(), x.end());
  int n = x.size();
  x.push_back(R_PosInf);
  double results = 0;
  int i = 1;
  int j = 0;
  while(j < n-1){
    if(i <= j || x[i+1] - x[j] < 2*r){
      ++i;
    } else if (x[i] - x[j] < 2*r) {
      for(int k = j+1; k <= i; ++k){
        results -= x[k];
      }
      results = results + (2*r + x[j]) * (i - j);
      ++j;
    } else {
      ++j;
    }
  }
  results = 2*results + 2*r*n;
  return(
    results * 9 / 4 / pow(n,2) / pow(r, 6)
  );
}
```

## Checking that results are correct{.scrollable .smaller}

```{r Running Tests for l2norm}
#| echo: true
devtools::test(filter="epanechnikov_l2norm")
```


```r
# Matrix-method
test_that("Matrix method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_matrix(0, 1),
    4.5
  )
})

test_that("Matrix method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 1),
    4.5 / l
  )
})

# Can't test the matrix method for 'real' data, since we have nothing to compare
# it to. It serves as our most trusted (and slowest) baseline

# Running
test_that("Running method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_running(0, 1),
    4.5
  )
})

test_that("Running method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_running(x, 1),
    4.5 / l
  )
})

test_that("Running method (real data)", {
  x <- rnorm(1000)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 0.1),
    epanechnikov_l2norm_running(x, 0.1)
  )
})

# Binning
test_that("Binning method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_binning(0, 1),
    4.5
  )
})

test_that("Binning method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_binning(x, 1),
    4.5 / l
  )
})

test_that("Binning method (real data)", {
  x <- rnorm(1000)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 0.1),
    epanechnikov_l2norm_binning(x, 0.1)
  )
})

# Cpp running
test_that("Running cpp method (singleton)", {
  expect_equal(
    epanechnikov_l2norm_runningC(0, 1),
    4.5
  )
})

test_that("Running cpp method (separate singletons)", {
  l <- 8
  x <- seq(4, 4*l, length.out = l)
  expect_equal(
    epanechnikov_l2norm_runningC(x, 1),
    4.5 / l
  )
})

test_that("Running cpp method (real data)", {
  x <- rnorm(1000)
  expect_equal(
    epanechnikov_l2norm_matrix(x, 0.1),
    epanechnikov_l2norm_runningC(x, 0.1)
  )
})
```

## Benchmarking $||f||_2^2$

```{r Benchmarking L2Norm}
#| cache: true
set.seed(0)
bm <- benchmark_epanechnikov_l2norm()
bm %>%
ggplot2::ggplot(ggplot2::aes(x = log(N), y = log(Time), color = Method)) +
  ggplot2::geom_line() +
  ggplot2::geom_point()
set.seed(NULL)
```





